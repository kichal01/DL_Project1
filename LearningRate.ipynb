{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3262ee12-f3a3-480d-ae93-de214ecbbc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torchvision.transforms import v2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f6b9b9-f0bd-4e1a-95a0-3d7580d14938",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_set = ImageFolder(root='cinic10/versions/1/train//', transform=transform)\n",
    "validate_set = ImageFolder(root='cinic10/versions/1/valid//', transform=transform)\n",
    "test_set = ImageFolder(root='cinic10/versions/1/test//', transform=transform)\n",
    "\n",
    "class_names = train_set.classes\n",
    "num_labels = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a23df68c-c866-487c-a5f4-d77054320092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "from torchvision.models import resnext50_32x4d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8f80e3-2ecf-41fd-9434-4d60ebd99b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_efficient_net(lr):\n",
    "    model = efficientnet_b0(pretrained=False)\n",
    "    model.classifier[1] = nn.Linear(in_features=1280, out_features=num_labels, bias=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb9b07b-bea7-4940-ac32-79dc52cecdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_default = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "def prepare_transforms():\n",
    "    \n",
    "    train_set = ImageFolder(root='cinic10/versions/1/train//', transform=transform_default)\n",
    "    validate_set = ImageFolder(root='cinic10/versions/1/valid//', transform=transform_default)\n",
    "    test_set = ImageFolder(root='cinic10/versions/1/test//', transform=transform_default)\n",
    "        \n",
    "    data_loader = DataLoader(train_set, batch_size=128, num_workers=6, generator=torch.Generator(device='cpu'),pin_memory=True, shuffle=True,persistent_workers=True, prefetch_factor=4)\n",
    "    data_loader_val = DataLoader(validate_set, batch_size=128, num_workers=6, generator=torch.Generator(device='cpu'),pin_memory=True, shuffle=True,persistent_workers=True, prefetch_factor=4)\n",
    "    data_loader_test = DataLoader(test_set, batch_size=64, num_workers=4, generator=torch.Generator(device='cpu'),persistent_workers=True)\n",
    "\n",
    "    return data_loader, data_loader_val, data_loader_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b4699a4-5985-4d53-989c-368359474ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(lr, prepare_model, seed):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "    losses_tr = []\n",
    "    accuracies_tr = []\n",
    "    losses_val = []\n",
    "    accuracies_val = []\n",
    "    \n",
    "    for lrate in lr:\n",
    "        data_loader, data_loader_val, data_loader_test = prepare_transforms()\n",
    "        print('LearningRate: '+ str(lrate))\n",
    "    \n",
    "        model, optimizer, criterion = prepare_model(lrate)\n",
    "        \n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        num_epochs = 20\n",
    "        \n",
    "        prev_prev_loss = float('inf')\n",
    "        prev_loss = float('inf')\n",
    "        curr_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(num_epochs): \n",
    "            print('Entered the loop')\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for i, (batch_X, batch_Y) in enumerate(data_loader):\n",
    "                X = batch_X.to(device, non_blocking=True)  \n",
    "                if(i == 1):\n",
    "                    print('going')\n",
    "                Y = batch_Y.to(device, non_blocking=True)  \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, Y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == Y).sum().item()\n",
    "                total += Y.size(0)\n",
    "            \n",
    "\n",
    "            avg_loss = total_loss / len(data_loader)\n",
    "            train_accuracy = correct / total * 100\n",
    "            train_losses.append(avg_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_Y in data_loader_val:\n",
    "                    X = batch_X.to(device, non_blocking=True)\n",
    "                    Y = batch_Y.to(device, non_blocking=True)\n",
    "                    outputs = model(X)\n",
    "                    loss = criterion(outputs, Y)\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    correct += (predicted == Y).sum().item()\n",
    "                    total += Y.size(0)\n",
    "            avg_val_loss = val_loss / len(data_loader_val)\n",
    "            val_accuracy = correct / total * 100\n",
    "            print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "            \n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    \n",
    "            prev_prev_loss = prev_loss\n",
    "            prev_loss = curr_loss\n",
    "            curr_loss = avg_val_loss\n",
    "        \n",
    "            if(epoch == num_epochs - 1 ):\n",
    "                losses_tr.append(train_losses)\n",
    "                accuracies_tr.append(train_accuracies) \n",
    "                losses_val.append(val_losses)  \n",
    "                accuracies_val.append(val_accuracies) \n",
    "    return losses_tr, accuracies_tr, losses_val, accuracies_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd15d5ef-16fe-4de4-a554-0114035c51b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearningRate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\torch_envi\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\envs\\torch_envi\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered the loop\n",
      "going\n",
      "Epoch 1, Average Loss: 1.8964\n",
      "Epoch 1, Validation Loss: 1.7583, Accuracy: 36.44%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 2, Average Loss: 1.5984\n",
      "Epoch 2, Validation Loss: 1.4898, Accuracy: 45.37%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 3, Average Loss: 1.4447\n",
      "Epoch 3, Validation Loss: 1.4325, Accuracy: 47.85%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 4, Average Loss: 1.3392\n",
      "Epoch 4, Validation Loss: 1.3124, Accuracy: 52.66%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 5, Average Loss: 1.2503\n",
      "Epoch 5, Validation Loss: 1.4698, Accuracy: 47.10%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 6, Average Loss: 1.2847\n",
      "Epoch 6, Validation Loss: 1.2562, Accuracy: 55.05%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 7, Average Loss: 1.1661\n",
      "Epoch 7, Validation Loss: 1.1980, Accuracy: 56.72%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 8, Average Loss: 1.1227\n",
      "Epoch 8, Validation Loss: 1.1613, Accuracy: 58.01%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 9, Average Loss: 1.0766\n",
      "Epoch 9, Validation Loss: 1.1644, Accuracy: 58.07%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 10, Average Loss: 1.0803\n",
      "Epoch 10, Validation Loss: 1.1305, Accuracy: 59.63%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 11, Average Loss: 1.0993\n",
      "Epoch 11, Validation Loss: 1.1272, Accuracy: 59.65%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 12, Average Loss: 1.0523\n",
      "Epoch 12, Validation Loss: 1.3749, Accuracy: 50.82%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 13, Average Loss: 1.0453\n",
      "Epoch 13, Validation Loss: 1.0902, Accuracy: 60.72%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 14, Average Loss: 0.9548\n",
      "Epoch 14, Validation Loss: 1.0750, Accuracy: 61.35%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 15, Average Loss: 0.9503\n",
      "Epoch 15, Validation Loss: 1.0852, Accuracy: 61.11%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 16, Average Loss: 0.9024\n",
      "Epoch 16, Validation Loss: 1.0659, Accuracy: 62.06%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 17, Average Loss: 0.8815\n",
      "Epoch 17, Validation Loss: 1.0883, Accuracy: 61.00%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 18, Average Loss: 0.9859\n",
      "Epoch 18, Validation Loss: 1.0532, Accuracy: 62.79%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 19, Average Loss: 0.8746\n",
      "Epoch 19, Validation Loss: 1.0603, Accuracy: 62.64%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 20, Average Loss: 0.8037\n",
      "Epoch 20, Validation Loss: 1.0833, Accuracy: 62.29%\n",
      "LearningRate: 0.0005\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 1, Average Loss: 1.9730\n",
      "Epoch 1, Validation Loss: 1.7555, Accuracy: 34.95%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 2, Average Loss: 1.6737\n",
      "Epoch 2, Validation Loss: 1.5960, Accuracy: 41.45%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 3, Average Loss: 1.5185\n",
      "Epoch 3, Validation Loss: 1.5179, Accuracy: 44.83%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 4, Average Loss: 1.4281\n",
      "Epoch 4, Validation Loss: 1.4252, Accuracy: 48.06%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 5, Average Loss: 1.3223\n",
      "Epoch 5, Validation Loss: 1.3386, Accuracy: 51.46%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 6, Average Loss: 1.2527\n",
      "Epoch 6, Validation Loss: 1.2717, Accuracy: 53.86%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 7, Average Loss: 1.1929\n",
      "Epoch 7, Validation Loss: 1.2775, Accuracy: 53.68%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 8, Average Loss: 1.1457\n",
      "Epoch 8, Validation Loss: 1.2269, Accuracy: 55.57%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 9, Average Loss: 1.0868\n",
      "Epoch 9, Validation Loss: 1.2205, Accuracy: 56.14%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 10, Average Loss: 1.0433\n",
      "Epoch 10, Validation Loss: 1.1889, Accuracy: 57.36%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 11, Average Loss: 0.9994\n",
      "Epoch 11, Validation Loss: 1.1998, Accuracy: 57.36%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 12, Average Loss: 0.9502\n",
      "Epoch 12, Validation Loss: 1.2658, Accuracy: 55.05%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 13, Average Loss: 1.0934\n",
      "Epoch 13, Validation Loss: 1.1828, Accuracy: 57.67%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 14, Average Loss: 0.9256\n",
      "Epoch 14, Validation Loss: 1.1742, Accuracy: 58.57%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 15, Average Loss: 0.8578\n",
      "Epoch 15, Validation Loss: 1.2100, Accuracy: 57.91%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 16, Average Loss: 0.8261\n",
      "Epoch 16, Validation Loss: 1.1986, Accuracy: 58.45%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 17, Average Loss: 0.7823\n",
      "Epoch 17, Validation Loss: 1.2484, Accuracy: 57.52%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 18, Average Loss: 0.7435\n",
      "Epoch 18, Validation Loss: 1.2596, Accuracy: 57.78%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 19, Average Loss: 0.7272\n",
      "Epoch 19, Validation Loss: 1.2397, Accuracy: 58.08%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 20, Average Loss: 0.6822\n",
      "Epoch 20, Validation Loss: 1.2892, Accuracy: 57.99%\n",
      "LearningRate: 0.0001\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 1, Average Loss: 2.2000\n",
      "Epoch 1, Validation Loss: 2.1413, Accuracy: 21.83%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 2, Average Loss: 2.0370\n",
      "Epoch 2, Validation Loss: 1.9808, Accuracy: 27.39%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 3, Average Loss: 1.9059\n",
      "Epoch 3, Validation Loss: 1.8660, Accuracy: 30.89%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 4, Average Loss: 1.8055\n",
      "Epoch 4, Validation Loss: 1.7901, Accuracy: 34.58%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 5, Average Loss: 1.7312\n",
      "Epoch 5, Validation Loss: 1.7346, Accuracy: 36.29%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 6, Average Loss: 1.6681\n",
      "Epoch 6, Validation Loss: 1.6738, Accuracy: 38.72%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 7, Average Loss: 1.6099\n",
      "Epoch 7, Validation Loss: 1.6150, Accuracy: 40.39%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 8, Average Loss: 1.5560\n",
      "Epoch 8, Validation Loss: 1.5675, Accuracy: 42.61%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 9, Average Loss: 1.5049\n",
      "Epoch 9, Validation Loss: 1.5362, Accuracy: 43.77%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 10, Average Loss: 1.4563\n",
      "Epoch 10, Validation Loss: 1.5193, Accuracy: 44.74%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 11, Average Loss: 1.4150\n",
      "Epoch 11, Validation Loss: 1.4880, Accuracy: 45.57%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 12, Average Loss: 1.3731\n",
      "Epoch 12, Validation Loss: 1.4852, Accuracy: 45.56%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 13, Average Loss: 1.3340\n",
      "Epoch 13, Validation Loss: 1.4619, Accuracy: 46.81%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 14, Average Loss: 1.2936\n",
      "Epoch 14, Validation Loss: 1.4571, Accuracy: 47.03%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 15, Average Loss: 1.2532\n",
      "Epoch 15, Validation Loss: 1.4565, Accuracy: 47.32%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 16, Average Loss: 1.2158\n",
      "Epoch 16, Validation Loss: 1.4618, Accuracy: 47.50%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 17, Average Loss: 1.1762\n",
      "Epoch 17, Validation Loss: 1.4886, Accuracy: 47.04%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 18, Average Loss: 1.1453\n",
      "Epoch 18, Validation Loss: 1.4765, Accuracy: 47.47%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 19, Average Loss: 1.1051\n",
      "Epoch 19, Validation Loss: 1.4975, Accuracy: 47.56%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 20, Average Loss: 1.0700\n",
      "Epoch 20, Validation Loss: 1.5298, Accuracy: 47.07%\n",
      "LearningRate: 0.001\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 1, Average Loss: 1.9029\n",
      "Epoch 1, Validation Loss: 1.7396, Accuracy: 35.92%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 2, Average Loss: 1.6160\n",
      "Epoch 2, Validation Loss: 1.5445, Accuracy: 43.33%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 3, Average Loss: 1.4550\n",
      "Epoch 3, Validation Loss: 1.5111, Accuracy: 47.36%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 4, Average Loss: 1.3514\n",
      "Epoch 4, Validation Loss: 1.3574, Accuracy: 50.69%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 5, Average Loss: 1.2668\n",
      "Epoch 5, Validation Loss: 1.3170, Accuracy: 52.21%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 6, Average Loss: 1.2101\n",
      "Epoch 6, Validation Loss: 1.2599, Accuracy: 53.79%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 7, Average Loss: 1.1541\n",
      "Epoch 7, Validation Loss: 1.2959, Accuracy: 52.73%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 8, Average Loss: 1.1025\n",
      "Epoch 8, Validation Loss: 1.1522, Accuracy: 58.50%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 9, Average Loss: 1.0413\n",
      "Epoch 9, Validation Loss: 1.1383, Accuracy: 58.96%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 10, Average Loss: 1.0031\n",
      "Epoch 10, Validation Loss: 1.1360, Accuracy: 59.27%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 11, Average Loss: 0.9653\n",
      "Epoch 11, Validation Loss: 1.0865, Accuracy: 61.06%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 12, Average Loss: 0.9281\n",
      "Epoch 12, Validation Loss: 1.1180, Accuracy: 60.14%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 13, Average Loss: 0.9057\n",
      "Epoch 13, Validation Loss: 1.6397, Accuracy: 42.73%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 14, Average Loss: 0.9546\n",
      "Epoch 14, Validation Loss: 1.1970, Accuracy: 57.41%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 15, Average Loss: 0.8938\n",
      "Epoch 15, Validation Loss: 1.1304, Accuracy: 59.97%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 16, Average Loss: 0.8557\n",
      "Epoch 16, Validation Loss: 1.0531, Accuracy: 62.86%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 17, Average Loss: 0.7702\n",
      "Epoch 17, Validation Loss: 1.0692, Accuracy: 62.59%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 18, Average Loss: 0.7388\n",
      "Epoch 18, Validation Loss: 1.0446, Accuracy: 63.60%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 19, Average Loss: 0.7061\n",
      "Epoch 19, Validation Loss: 1.0878, Accuracy: 62.65%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 20, Average Loss: 0.7153\n",
      "Epoch 20, Validation Loss: 1.0678, Accuracy: 63.32%\n",
      "LearningRate: 0.0005\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 1, Average Loss: 1.9906\n",
      "Epoch 1, Validation Loss: 1.7919, Accuracy: 33.10%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 2, Average Loss: 1.6762\n",
      "Epoch 2, Validation Loss: 1.5941, Accuracy: 41.08%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 3, Average Loss: 1.5221\n",
      "Epoch 3, Validation Loss: 1.4907, Accuracy: 45.54%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 4, Average Loss: 1.4145\n",
      "Epoch 4, Validation Loss: 1.4233, Accuracy: 48.06%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 5, Average Loss: 1.3252\n",
      "Epoch 5, Validation Loss: 1.3450, Accuracy: 50.70%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 6, Average Loss: 1.2585\n",
      "Epoch 6, Validation Loss: 1.2947, Accuracy: 53.07%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 7, Average Loss: 1.1979\n",
      "Epoch 7, Validation Loss: 1.2931, Accuracy: 53.14%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 8, Average Loss: 1.1448\n",
      "Epoch 8, Validation Loss: 1.2280, Accuracy: 55.49%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 9, Average Loss: 1.0917\n",
      "Epoch 9, Validation Loss: 1.2302, Accuracy: 55.79%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 10, Average Loss: 1.0486\n",
      "Epoch 10, Validation Loss: 1.3119, Accuracy: 53.36%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 11, Average Loss: 1.0173\n",
      "Epoch 11, Validation Loss: 1.1938, Accuracy: 57.64%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 12, Average Loss: 0.9581\n",
      "Epoch 12, Validation Loss: 1.2139, Accuracy: 57.08%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 13, Average Loss: 0.9208\n",
      "Epoch 13, Validation Loss: 1.2257, Accuracy: 56.68%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 14, Average Loss: 0.8689\n",
      "Epoch 14, Validation Loss: 1.2091, Accuracy: 57.21%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 15, Average Loss: 0.8365\n",
      "Epoch 15, Validation Loss: 1.2398, Accuracy: 57.34%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 16, Average Loss: 0.7966\n",
      "Epoch 16, Validation Loss: 1.2665, Accuracy: 57.36%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 17, Average Loss: 0.7672\n",
      "Epoch 17, Validation Loss: 1.2805, Accuracy: 57.01%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 18, Average Loss: 0.7241\n",
      "Epoch 18, Validation Loss: 1.2740, Accuracy: 57.70%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 19, Average Loss: 0.6776\n",
      "Epoch 19, Validation Loss: 1.2840, Accuracy: 57.42%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 20, Average Loss: 0.6511\n",
      "Epoch 20, Validation Loss: 1.3221, Accuracy: 57.29%\n",
      "LearningRate: 0.0001\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 1, Average Loss: 2.1765\n",
      "Epoch 1, Validation Loss: 2.0772, Accuracy: 23.65%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 2, Average Loss: 1.9824\n",
      "Epoch 2, Validation Loss: 1.9331, Accuracy: 29.14%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 3, Average Loss: 1.8510\n",
      "Epoch 3, Validation Loss: 1.8021, Accuracy: 32.96%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 4, Average Loss: 1.7591\n",
      "Epoch 4, Validation Loss: 1.7278, Accuracy: 36.19%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 5, Average Loss: 1.6865\n",
      "Epoch 5, Validation Loss: 1.6808, Accuracy: 38.28%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 6, Average Loss: 1.6198\n",
      "Epoch 6, Validation Loss: 1.6230, Accuracy: 40.46%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 7, Average Loss: 1.5645\n",
      "Epoch 7, Validation Loss: 1.5752, Accuracy: 41.68%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 8, Average Loss: 1.5079\n",
      "Epoch 8, Validation Loss: 1.5229, Accuracy: 43.86%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 9, Average Loss: 1.4595\n",
      "Epoch 9, Validation Loss: 1.5033, Accuracy: 44.74%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 10, Average Loss: 1.4103\n",
      "Epoch 10, Validation Loss: 1.4894, Accuracy: 45.43%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 11, Average Loss: 1.3685\n",
      "Epoch 11, Validation Loss: 1.4648, Accuracy: 46.51%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 12, Average Loss: 1.3267\n",
      "Epoch 12, Validation Loss: 1.4559, Accuracy: 46.97%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 13, Average Loss: 1.2863\n",
      "Epoch 13, Validation Loss: 1.4612, Accuracy: 47.07%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 14, Average Loss: 1.2475\n",
      "Epoch 14, Validation Loss: 1.4745, Accuracy: 46.97%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 15, Average Loss: 1.2058\n",
      "Epoch 15, Validation Loss: 1.4616, Accuracy: 47.46%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 16, Average Loss: 1.1706\n",
      "Epoch 16, Validation Loss: 1.4675, Accuracy: 47.63%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 17, Average Loss: 1.1283\n",
      "Epoch 17, Validation Loss: 1.4732, Accuracy: 47.61%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 18, Average Loss: 1.0932\n",
      "Epoch 18, Validation Loss: 1.4974, Accuracy: 47.45%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 19, Average Loss: 1.0569\n",
      "Epoch 19, Validation Loss: 1.5133, Accuracy: 47.48%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 20, Average Loss: 1.0217\n",
      "Epoch 20, Validation Loss: 1.5463, Accuracy: 47.49%\n",
      "LearningRate: 0.001\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 1, Average Loss: 1.9031\n",
      "Epoch 1, Validation Loss: 1.6832, Accuracy: 36.74%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 2, Average Loss: 1.6057\n",
      "Epoch 2, Validation Loss: 1.5348, Accuracy: 44.14%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 3, Average Loss: 1.4656\n",
      "Epoch 3, Validation Loss: 1.4370, Accuracy: 47.71%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 4, Average Loss: 1.3573\n",
      "Epoch 4, Validation Loss: 1.3412, Accuracy: 50.76%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 5, Average Loss: 1.2677\n",
      "Epoch 5, Validation Loss: 1.2959, Accuracy: 53.96%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 6, Average Loss: 1.1972\n",
      "Epoch 6, Validation Loss: 1.2309, Accuracy: 55.62%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 7, Average Loss: 1.1553\n",
      "Epoch 7, Validation Loss: 1.2136, Accuracy: 56.89%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 8, Average Loss: 1.0986\n",
      "Epoch 8, Validation Loss: 1.1758, Accuracy: 58.04%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 9, Average Loss: 1.0419\n",
      "Epoch 9, Validation Loss: 1.1571, Accuracy: 58.30%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 10, Average Loss: 1.0016\n",
      "Epoch 10, Validation Loss: 1.1997, Accuracy: 57.79%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 11, Average Loss: 0.9697\n",
      "Epoch 11, Validation Loss: 1.0980, Accuracy: 60.67%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 12, Average Loss: 1.0769\n",
      "Epoch 12, Validation Loss: 1.0936, Accuracy: 60.68%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 13, Average Loss: 0.9217\n",
      "Epoch 13, Validation Loss: 1.0854, Accuracy: 61.24%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 14, Average Loss: 0.8710\n",
      "Epoch 14, Validation Loss: 1.1045, Accuracy: 60.58%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 15, Average Loss: 0.8297\n",
      "Epoch 15, Validation Loss: 1.0633, Accuracy: 62.43%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 16, Average Loss: 0.8031\n",
      "Epoch 16, Validation Loss: 1.0520, Accuracy: 62.89%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 17, Average Loss: 0.7890\n",
      "Epoch 17, Validation Loss: 1.0667, Accuracy: 63.03%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 18, Average Loss: 0.9693\n",
      "Epoch 18, Validation Loss: 1.0643, Accuracy: 61.87%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 19, Average Loss: 0.8048\n",
      "Epoch 19, Validation Loss: 1.0334, Accuracy: 63.74%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 20, Average Loss: 0.7097\n",
      "Epoch 20, Validation Loss: 1.0620, Accuracy: 63.49%\n",
      "LearningRate: 0.0005\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 1, Average Loss: 1.9684\n",
      "Epoch 1, Validation Loss: 1.7597, Accuracy: 33.15%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 2, Average Loss: 1.6783\n",
      "Epoch 2, Validation Loss: 1.5996, Accuracy: 41.09%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 3, Average Loss: 1.5199\n",
      "Epoch 3, Validation Loss: 1.4484, Accuracy: 46.45%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 4, Average Loss: 1.4071\n",
      "Epoch 4, Validation Loss: 1.3996, Accuracy: 48.73%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 5, Average Loss: 1.3205\n",
      "Epoch 5, Validation Loss: 1.3466, Accuracy: 50.79%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 6, Average Loss: 1.2563\n",
      "Epoch 6, Validation Loss: 1.2929, Accuracy: 52.81%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 7, Average Loss: 1.2002\n",
      "Epoch 7, Validation Loss: 1.2820, Accuracy: 53.50%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 8, Average Loss: 1.1486\n",
      "Epoch 8, Validation Loss: 1.2431, Accuracy: 55.10%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 9, Average Loss: 1.0975\n",
      "Epoch 9, Validation Loss: 1.2267, Accuracy: 55.85%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 10, Average Loss: 1.0498\n",
      "Epoch 10, Validation Loss: 1.2188, Accuracy: 56.42%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 11, Average Loss: 1.0101\n",
      "Epoch 11, Validation Loss: 1.1872, Accuracy: 57.39%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 12, Average Loss: 0.9594\n",
      "Epoch 12, Validation Loss: 1.1932, Accuracy: 57.12%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 13, Average Loss: 0.9235\n",
      "Epoch 13, Validation Loss: 1.2012, Accuracy: 57.50%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 14, Average Loss: 0.8806\n",
      "Epoch 14, Validation Loss: 1.2328, Accuracy: 56.49%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 15, Average Loss: 0.8352\n",
      "Epoch 15, Validation Loss: 1.2253, Accuracy: 57.32%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 16, Average Loss: 0.8082\n",
      "Epoch 16, Validation Loss: 1.2302, Accuracy: 57.88%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 17, Average Loss: 0.7486\n",
      "Epoch 17, Validation Loss: 1.2821, Accuracy: 57.31%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 18, Average Loss: 0.7501\n",
      "Epoch 18, Validation Loss: 1.2453, Accuracy: 57.90%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 19, Average Loss: 0.6845\n",
      "Epoch 19, Validation Loss: 1.2686, Accuracy: 57.81%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 20, Average Loss: 0.6477\n",
      "Epoch 20, Validation Loss: 1.3143, Accuracy: 57.17%\n",
      "LearningRate: 0.0001\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 1, Average Loss: 2.2136\n",
      "Epoch 1, Validation Loss: 2.1105, Accuracy: 22.09%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 2, Average Loss: 2.0252\n",
      "Epoch 2, Validation Loss: 1.9577, Accuracy: 27.46%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 3, Average Loss: 1.8978\n",
      "Epoch 3, Validation Loss: 1.8353, Accuracy: 31.33%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 4, Average Loss: 1.7988\n",
      "Epoch 4, Validation Loss: 1.7603, Accuracy: 34.53%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 5, Average Loss: 1.7218\n",
      "Epoch 5, Validation Loss: 1.6973, Accuracy: 36.73%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 6, Average Loss: 1.6519\n",
      "Epoch 6, Validation Loss: 1.6460, Accuracy: 39.13%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 7, Average Loss: 1.5916\n",
      "Epoch 7, Validation Loss: 1.6011, Accuracy: 40.70%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 8, Average Loss: 1.5321\n",
      "Epoch 8, Validation Loss: 1.5493, Accuracy: 43.11%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 9, Average Loss: 1.4785\n",
      "Epoch 9, Validation Loss: 1.5295, Accuracy: 44.12%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 10, Average Loss: 1.4306\n",
      "Epoch 10, Validation Loss: 1.5036, Accuracy: 45.23%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 11, Average Loss: 1.3859\n",
      "Epoch 11, Validation Loss: 1.4800, Accuracy: 46.07%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 12, Average Loss: 1.3443\n",
      "Epoch 12, Validation Loss: 1.4755, Accuracy: 46.24%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 13, Average Loss: 1.3032\n",
      "Epoch 13, Validation Loss: 1.4698, Accuracy: 46.81%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 14, Average Loss: 1.2636\n",
      "Epoch 14, Validation Loss: 1.4627, Accuracy: 47.14%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 15, Average Loss: 1.2248\n",
      "Epoch 15, Validation Loss: 1.4868, Accuracy: 47.30%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 16, Average Loss: 1.1856\n",
      "Epoch 16, Validation Loss: 1.4822, Accuracy: 46.99%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 17, Average Loss: 1.1497\n",
      "Epoch 17, Validation Loss: 1.4925, Accuracy: 47.32%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 18, Average Loss: 1.1111\n",
      "Epoch 18, Validation Loss: 1.5044, Accuracy: 47.27%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 19, Average Loss: 1.0750\n",
      "Epoch 19, Validation Loss: 1.5209, Accuracy: 46.93%\n",
      "Entered the loop\n",
      "going\n",
      "Epoch 20, Average Loss: 1.0436\n",
      "Epoch 20, Validation Loss: 1.5335, Accuracy: 47.06%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "lr = [0.001, 0.0005, 0.0001]\n",
    "for i in range(3):\n",
    "     results.append(model_train(lr, prepare_model_efficient_net, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf1ee7a-7add-4707-a278-0ba31c58f29b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([[1.896396836435253,\n",
       "    1.5984010344201869,\n",
       "    1.4446623218669132,\n",
       "    1.3392044288868254,\n",
       "    1.2502555649896914,\n",
       "    1.2846737410873175,\n",
       "    1.1660954192788764,\n",
       "    1.1227205916392533,\n",
       "    1.0765761206434532,\n",
       "    1.0802768372337928,\n",
       "    1.0993167562410235,\n",
       "    1.0523471283641728,\n",
       "    1.0453071816062385,\n",
       "    0.9547779339111664,\n",
       "    0.9503065279938958,\n",
       "    0.9023698487911712,\n",
       "    0.8814615334638141,\n",
       "    0.9858805806460705,\n",
       "    0.8746040218763731,\n",
       "    0.8036879329857501],\n",
       "   [1.9730042593384332,\n",
       "    1.673728359863162,\n",
       "    1.5184885078871793,\n",
       "    1.428129217164083,\n",
       "    1.3223173137415538,\n",
       "    1.2527341944250194,\n",
       "    1.1928628363053908,\n",
       "    1.1457005945796317,\n",
       "    1.0867845576087183,\n",
       "    1.043328912505372,\n",
       "    0.9994076538661664,\n",
       "    0.9502070731568065,\n",
       "    1.0934111700308593,\n",
       "    0.9255639664320783,\n",
       "    0.8577949001707814,\n",
       "    0.8261074533008717,\n",
       "    0.7823491135442798,\n",
       "    0.7435439554974437,\n",
       "    0.7271724405105818,\n",
       "    0.6822288703088734],\n",
       "   [2.1999576154418965,\n",
       "    2.036997713487257,\n",
       "    1.9058921797709032,\n",
       "    1.80548307181082,\n",
       "    1.7311818911270662,\n",
       "    1.6680606175214052,\n",
       "    1.6099140315570615,\n",
       "    1.5559773363850333,\n",
       "    1.5048637412149797,\n",
       "    1.4562604356218467,\n",
       "    1.4150352652438662,\n",
       "    1.3730883474715732,\n",
       "    1.3340235518460923,\n",
       "    1.2935514483939519,\n",
       "    1.2532148046249694,\n",
       "    1.2157599876888774,\n",
       "    1.1762289912016555,\n",
       "    1.1453021862120791,\n",
       "    1.1051385964859615,\n",
       "    1.0700272382660345]],\n",
       "  [[28.925555555555555,\n",
       "    41.455555555555556,\n",
       "    47.45111111111111,\n",
       "    51.36222222222222,\n",
       "    54.59444444444445,\n",
       "    53.14111111111111,\n",
       "    57.56333333333333,\n",
       "    59.52222222222222,\n",
       "    61.191111111111105,\n",
       "    61.06222222222222,\n",
       "    60.58222222222223,\n",
       "    62.28111111111111,\n",
       "    62.480000000000004,\n",
       "    65.59555555555555,\n",
       "    65.75,\n",
       "    67.68666666666667,\n",
       "    68.33222222222223,\n",
       "    64.69222222222221,\n",
       "    68.72777777777777,\n",
       "    71.33888888888889],\n",
       "   [26.24666666666667,\n",
       "    38.251111111111115,\n",
       "    44.37,\n",
       "    47.70111111111111,\n",
       "    51.83777777777778,\n",
       "    54.40555555555555,\n",
       "    56.80222222222222,\n",
       "    58.50222222222222,\n",
       "    60.77111111111111,\n",
       "    62.321111111111115,\n",
       "    64.15555555555555,\n",
       "    65.76666666666667,\n",
       "    60.523333333333326,\n",
       "    66.63,\n",
       "    69.2388888888889,\n",
       "    70.32111111111111,\n",
       "    71.81111111111112,\n",
       "    73.42,\n",
       "    73.98,\n",
       "    75.49],\n",
       "   [17.737777777777776,\n",
       "    23.263333333333332,\n",
       "    28.445555555555558,\n",
       "    32.45666666666666,\n",
       "    35.681111111111115,\n",
       "    38.205555555555556,\n",
       "    40.38888888888889,\n",
       "    42.647777777777776,\n",
       "    44.7,\n",
       "    46.58,\n",
       "    48.20888888888889,\n",
       "    49.86666666666667,\n",
       "    51.303333333333335,\n",
       "    52.99333333333334,\n",
       "    54.40888888888888,\n",
       "    55.735555555555564,\n",
       "    57.257777777777775,\n",
       "    58.20444444444445,\n",
       "    59.78333333333333,\n",
       "    61.05777777777778]],\n",
       "  [[1.7582565525716,\n",
       "    1.4898140215399591,\n",
       "    1.4325477445328778,\n",
       "    1.3124373937872322,\n",
       "    1.4697639283470132,\n",
       "    1.2562347291545435,\n",
       "    1.198049275289205,\n",
       "    1.161290788684379,\n",
       "    1.1643869027664715,\n",
       "    1.130526982332495,\n",
       "    1.1271639033644036,\n",
       "    1.374859324910424,\n",
       "    1.090209410356527,\n",
       "    1.0750299643894488,\n",
       "    1.0851601476527073,\n",
       "    1.0658898859877477,\n",
       "    1.088328724049709,\n",
       "    1.0532382807948373,\n",
       "    1.0602890900759534,\n",
       "    1.0833041325042194],\n",
       "   [1.7555259080095724,\n",
       "    1.5960067566484213,\n",
       "    1.51785083132034,\n",
       "    1.4252394507215782,\n",
       "    1.3386427792297169,\n",
       "    1.271702436391603,\n",
       "    1.2775048856538804,\n",
       "    1.2268696267327124,\n",
       "    1.2205084269358353,\n",
       "    1.1889303796501323,\n",
       "    1.1997902643104845,\n",
       "    1.2657743938775226,\n",
       "    1.182824136773971,\n",
       "    1.1741877617314458,\n",
       "    1.2099740477278829,\n",
       "    1.1986372851851312,\n",
       "    1.2484496300193397,\n",
       "    1.2595862227238037,\n",
       "    1.2396938123486259,\n",
       "    1.289155465212058],\n",
       "   [2.141269875351678,\n",
       "    1.9807509801943193,\n",
       "    1.8660445430062034,\n",
       "    1.7901280930435115,\n",
       "    1.7345891266383908,\n",
       "    1.6738081132485108,\n",
       "    1.6150269911370494,\n",
       "    1.5674923409453847,\n",
       "    1.5362024769525637,\n",
       "    1.5192668786780401,\n",
       "    1.4879595994610677,\n",
       "    1.4851904225281694,\n",
       "    1.4619058448482642,\n",
       "    1.4570862439207055,\n",
       "    1.4565034569664435,\n",
       "    1.4618274176662618,\n",
       "    1.4885634752837094,\n",
       "    1.4765080951831557,\n",
       "    1.4974810539490797,\n",
       "    1.529768471267413]],\n",
       "  [[36.44,\n",
       "    45.37,\n",
       "    47.84777777777778,\n",
       "    52.65555555555556,\n",
       "    47.09777777777778,\n",
       "    55.04555555555556,\n",
       "    56.718888888888884,\n",
       "    58.01444444444444,\n",
       "    58.06888888888889,\n",
       "    59.63222222222222,\n",
       "    59.648888888888884,\n",
       "    50.81888888888889,\n",
       "    60.72333333333333,\n",
       "    61.35444444444445,\n",
       "    61.10666666666666,\n",
       "    62.05888888888889,\n",
       "    61.00444444444444,\n",
       "    62.794444444444444,\n",
       "    62.641111111111115,\n",
       "    62.28555555555556],\n",
       "   [34.95444444444444,\n",
       "    41.44555555555556,\n",
       "    44.83111111111111,\n",
       "    48.06,\n",
       "    51.46222222222222,\n",
       "    53.855555555555554,\n",
       "    53.684444444444445,\n",
       "    55.571111111111115,\n",
       "    56.13555555555556,\n",
       "    57.36444444444444,\n",
       "    57.36222222222223,\n",
       "    55.05444444444444,\n",
       "    57.668888888888894,\n",
       "    58.56888888888889,\n",
       "    57.90555555555555,\n",
       "    58.44888888888888,\n",
       "    57.51777777777778,\n",
       "    57.77777777777777,\n",
       "    58.08333333333333,\n",
       "    57.99333333333333],\n",
       "   [21.83222222222222,\n",
       "    27.392222222222223,\n",
       "    30.88888888888889,\n",
       "    34.583333333333336,\n",
       "    36.28666666666667,\n",
       "    38.71666666666667,\n",
       "    40.39111111111111,\n",
       "    42.60777777777778,\n",
       "    43.766666666666666,\n",
       "    44.74,\n",
       "    45.57,\n",
       "    45.56111111111111,\n",
       "    46.81111111111111,\n",
       "    47.034444444444446,\n",
       "    47.31666666666667,\n",
       "    47.49666666666666,\n",
       "    47.04333333333333,\n",
       "    47.46888888888889,\n",
       "    47.55666666666667,\n",
       "    47.07222222222222]]),\n",
       " ([[1.9029125156389042,\n",
       "    1.6160081911154769,\n",
       "    1.4550291311673142,\n",
       "    1.3514123431999574,\n",
       "    1.2668104964223774,\n",
       "    1.2101388875903054,\n",
       "    1.1541284995830872,\n",
       "    1.1024725944311782,\n",
       "    1.0412954057312824,\n",
       "    1.0031208061528476,\n",
       "    0.965295550328764,\n",
       "    0.928055983460085,\n",
       "    0.9057005208662965,\n",
       "    0.9545861282809214,\n",
       "    0.8937770383093845,\n",
       "    0.8557017673738301,\n",
       "    0.7702415347607299,\n",
       "    0.738813419487666,\n",
       "    0.7060721094974063,\n",
       "    0.7152775383808396],\n",
       "   [1.9905672681263902,\n",
       "    1.6761672159826213,\n",
       "    1.5221340752799402,\n",
       "    1.414467248388312,\n",
       "    1.3251674632457169,\n",
       "    1.2584881514988162,\n",
       "    1.1979166563092307,\n",
       "    1.1447868573225357,\n",
       "    1.0916724739257584,\n",
       "    1.0485736182467504,\n",
       "    1.0173270262947136,\n",
       "    0.9580957462841814,\n",
       "    0.9207620242271911,\n",
       "    0.8689406565813855,\n",
       "    0.8364553883333098,\n",
       "    0.7965667024254799,\n",
       "    0.767190564296801,\n",
       "    0.7240925639952448,\n",
       "    0.6775614272549071,\n",
       "    0.6511177194558762],\n",
       "   [2.17651907320727,\n",
       "    1.98243415186351,\n",
       "    1.8509824560447172,\n",
       "    1.7590928633104672,\n",
       "    1.6864627183160998,\n",
       "    1.6197540114887736,\n",
       "    1.5644873621111566,\n",
       "    1.5079151767898689,\n",
       "    1.4594715217297727,\n",
       "    1.4102570670233532,\n",
       "    1.3684714890339158,\n",
       "    1.326732806184075,\n",
       "    1.2862549092281947,\n",
       "    1.2475000166080215,\n",
       "    1.2057695939967579,\n",
       "    1.1705697012895888,\n",
       "    1.128301264718175,\n",
       "    1.0931849972429601,\n",
       "    1.056915964592587,\n",
       "    1.0217389908873222]],\n",
       "  [[28.89888888888889,\n",
       "    40.346666666666664,\n",
       "    47.03666666666667,\n",
       "    50.89666666666667,\n",
       "    54.245555555555555,\n",
       "    56.20333333333334,\n",
       "    58.51,\n",
       "    60.43777777777778,\n",
       "    62.87222222222222,\n",
       "    64.3311111111111,\n",
       "    65.63,\n",
       "    67.07111111111111,\n",
       "    67.76222222222222,\n",
       "    66.00111111111111,\n",
       "    68.47888888888889,\n",
       "    69.73222222222222,\n",
       "    72.61111111111111,\n",
       "    73.79444444444444,\n",
       "    74.8888888888889,\n",
       "    74.63666666666666],\n",
       "   [25.453333333333333,\n",
       "    37.84888888888889,\n",
       "    43.983333333333334,\n",
       "    48.056666666666665,\n",
       "    51.60444444444444,\n",
       "    54.123333333333335,\n",
       "    56.544444444444444,\n",
       "    58.41444444444445,\n",
       "    60.501111111111115,\n",
       "    62.19222222222223,\n",
       "    63.33555555555556,\n",
       "    65.57333333333332,\n",
       "    66.79666666666667,\n",
       "    68.60555555555555,\n",
       "    69.83444444444444,\n",
       "    71.51111111111112,\n",
       "    72.35000000000001,\n",
       "    73.99555555555555,\n",
       "    75.52333333333333,\n",
       "    76.75777777777778],\n",
       "   [18.18111111111111,\n",
       "    25.288888888888888,\n",
       "    30.422222222222224,\n",
       "    34.17111111111112,\n",
       "    37.45888888888889,\n",
       "    39.75888888888889,\n",
       "    42.27111111111111,\n",
       "    44.593333333333334,\n",
       "    46.57555555555555,\n",
       "    48.352222222222224,\n",
       "    49.964444444444446,\n",
       "    51.431111111111115,\n",
       "    53.03666666666666,\n",
       "    54.556666666666665,\n",
       "    56.17888888888889,\n",
       "    57.382222222222225,\n",
       "    59.10333333333333,\n",
       "    60.215555555555554,\n",
       "    61.577777777777776,\n",
       "    63.10777777777778]],\n",
       "  [[1.739557828076861,\n",
       "    1.5445075692100958,\n",
       "    1.511095880446109,\n",
       "    1.3574172845956953,\n",
       "    1.317041534253142,\n",
       "    1.259886889718473,\n",
       "    1.29590558599342,\n",
       "    1.1521560052748432,\n",
       "    1.1382523092695258,\n",
       "    1.1360199920494447,\n",
       "    1.086541687053713,\n",
       "    1.1179858960041946,\n",
       "    1.639666000381112,\n",
       "    1.1970453843135724,\n",
       "    1.1303998311473564,\n",
       "    1.0531103602525862,\n",
       "    1.0692381800406359,\n",
       "    1.0446439684284003,\n",
       "    1.087837777523832,\n",
       "    1.0678382250057026],\n",
       "   [1.7918829076330771,\n",
       "    1.5940833591263404,\n",
       "    1.4906625605442307,\n",
       "    1.4233207020231269,\n",
       "    1.3450389233502476,\n",
       "    1.2947106022726407,\n",
       "    1.2931435497646981,\n",
       "    1.227961908602579,\n",
       "    1.2301910098811442,\n",
       "    1.311948843469674,\n",
       "    1.1938342998989604,\n",
       "    1.2139336927370592,\n",
       "    1.225703271275217,\n",
       "    1.2090821300040593,\n",
       "    1.2398264723067933,\n",
       "    1.266512662341649,\n",
       "    1.2804938470098106,\n",
       "    1.2740336648103865,\n",
       "    1.2839771255173467,\n",
       "    1.3220876168290323],\n",
       "   [2.0772399809211493,\n",
       "    1.9331369124014268,\n",
       "    1.8020962305705657,\n",
       "    1.7278237456286496,\n",
       "    1.6808017546480352,\n",
       "    1.6230148048224775,\n",
       "    1.5751678621904417,\n",
       "    1.5228511332110926,\n",
       "    1.5033111580732195,\n",
       "    1.489406787536361,\n",
       "    1.464775749058886,\n",
       "    1.4558880254626274,\n",
       "    1.4612106989053162,\n",
       "    1.4745141997594724,\n",
       "    1.4616364943371578,\n",
       "    1.4674613130363552,\n",
       "    1.4731975180858916,\n",
       "    1.4973639811981807,\n",
       "    1.5132731231437488,\n",
       "    1.5462987597354434]],\n",
       "  [[35.92333333333333,\n",
       "    43.333333333333336,\n",
       "    47.36333333333334,\n",
       "    50.690000000000005,\n",
       "    52.21111111111111,\n",
       "    53.79333333333334,\n",
       "    52.733333333333334,\n",
       "    58.50222222222222,\n",
       "    58.96111111111111,\n",
       "    59.27111111111111,\n",
       "    61.06,\n",
       "    60.142222222222216,\n",
       "    42.72555555555556,\n",
       "    57.410000000000004,\n",
       "    59.96555555555556,\n",
       "    62.85666666666667,\n",
       "    62.59,\n",
       "    63.59777777777778,\n",
       "    62.64888888888889,\n",
       "    63.315555555555555],\n",
       "   [33.09888888888889,\n",
       "    41.07888888888888,\n",
       "    45.53666666666666,\n",
       "    48.05555555555556,\n",
       "    50.70111111111111,\n",
       "    53.06666666666666,\n",
       "    53.14111111111111,\n",
       "    55.489999999999995,\n",
       "    55.78666666666666,\n",
       "    53.357777777777784,\n",
       "    57.63666666666667,\n",
       "    57.08444444444445,\n",
       "    56.68,\n",
       "    57.214444444444446,\n",
       "    57.337777777777774,\n",
       "    57.36333333333333,\n",
       "    57.010000000000005,\n",
       "    57.69555555555556,\n",
       "    57.41888888888889,\n",
       "    57.28666666666666],\n",
       "   [23.65,\n",
       "    29.141111111111112,\n",
       "    32.958888888888886,\n",
       "    36.18777777777778,\n",
       "    38.28111111111111,\n",
       "    40.458888888888886,\n",
       "    41.68333333333333,\n",
       "    43.858888888888885,\n",
       "    44.73777777777778,\n",
       "    45.43,\n",
       "    46.507777777777775,\n",
       "    46.96666666666667,\n",
       "    47.07,\n",
       "    46.97222222222222,\n",
       "    47.46333333333334,\n",
       "    47.63,\n",
       "    47.61222222222222,\n",
       "    47.449999999999996,\n",
       "    47.47888888888889,\n",
       "    47.486666666666665]]),\n",
       " ([[1.9031229715117,\n",
       "    1.605719853700562,\n",
       "    1.4656332006508654,\n",
       "    1.3572791496461087,\n",
       "    1.2676705614748327,\n",
       "    1.1971611379730431,\n",
       "    1.1552939240566709,\n",
       "    1.0986201608214865,\n",
       "    1.0419135077602484,\n",
       "    1.0015589897097512,\n",
       "    0.9697379058396275,\n",
       "    1.0768553736534985,\n",
       "    0.9217383295128291,\n",
       "    0.8710194328291849,\n",
       "    0.8296642829240723,\n",
       "    0.8031217682767998,\n",
       "    0.7890431263738058,\n",
       "    0.969265600073744,\n",
       "    0.8048368326642297,\n",
       "    0.7096513438631188],\n",
       "   [1.9683958918533542,\n",
       "    1.6782684593715451,\n",
       "    1.5199272204190493,\n",
       "    1.4071151759814133,\n",
       "    1.3205126993019471,\n",
       "    1.2563448969952085,\n",
       "    1.2001577834175392,\n",
       "    1.1486050329086455,\n",
       "    1.097450076433068,\n",
       "    1.0498153013600544,\n",
       "    1.0100518399002878,\n",
       "    0.9594201100990176,\n",
       "    0.9234926957129077,\n",
       "    0.880564124492759,\n",
       "    0.8352296864613891,\n",
       "    0.8082168905741789,\n",
       "    0.7485891043801199,\n",
       "    0.7500807570970871,\n",
       "    0.6845313248393888,\n",
       "    0.6477399846454236],\n",
       "   [2.2135701013559643,\n",
       "    2.0252498281611637,\n",
       "    1.8977825878696009,\n",
       "    1.798800259151242,\n",
       "    1.7218194488774647,\n",
       "    1.6518539903177456,\n",
       "    1.5916221449998291,\n",
       "    1.5320529460229657,\n",
       "    1.4785358309745789,\n",
       "    1.4305507345971735,\n",
       "    1.3859191345558926,\n",
       "    1.344271451912143,\n",
       "    1.3031799525699832,\n",
       "    1.2635634573684498,\n",
       "    1.22483461773531,\n",
       "    1.1856140702624212,\n",
       "    1.1496807379140095,\n",
       "    1.1111250506205992,\n",
       "    1.0750345143235542,\n",
       "    1.0435933274301616]],\n",
       "  [[29.085555555555555,\n",
       "    40.90111111111111,\n",
       "    46.488888888888894,\n",
       "    50.602222222222224,\n",
       "    54.34444444444444,\n",
       "    56.782222222222224,\n",
       "    58.49888888888889,\n",
       "    60.56555555555555,\n",
       "    62.60555555555556,\n",
       "    64.22333333333333,\n",
       "    65.56777777777778,\n",
       "    61.68,\n",
       "    67.12222222222223,\n",
       "    68.89222222222222,\n",
       "    70.41333333333334,\n",
       "    71.42777777777778,\n",
       "    71.92888888888889,\n",
       "    65.47666666666667,\n",
       "    71.22222222222221,\n",
       "    74.65222222222222],\n",
       "   [24.935555555555556,\n",
       "    37.12888888888889,\n",
       "    43.73888888888889,\n",
       "    48.20111111111111,\n",
       "    51.66666666666667,\n",
       "    54.19555555555555,\n",
       "    56.60666666666667,\n",
       "    58.42333333333334,\n",
       "    60.39333333333333,\n",
       "    62.05666666666667,\n",
       "    63.54444444444445,\n",
       "    65.40555555555555,\n",
       "    66.73777777777778,\n",
       "    68.21555555555555,\n",
       "    69.84,\n",
       "    70.87222222222222,\n",
       "    73.09555555555556,\n",
       "    72.9688888888889,\n",
       "    75.35222222222222,\n",
       "    76.68333333333334],\n",
       "   [17.401111111111113,\n",
       "    24.05777777777778,\n",
       "    28.984444444444446,\n",
       "    32.41777777777778,\n",
       "    35.77444444444444,\n",
       "    38.62111111111111,\n",
       "    41.22,\n",
       "    43.587777777777774,\n",
       "    45.83444444444444,\n",
       "    47.57333333333334,\n",
       "    49.28444444444444,\n",
       "    50.83111111111111,\n",
       "    52.39,\n",
       "    53.898888888888884,\n",
       "    55.51777777777778,\n",
       "    56.787777777777784,\n",
       "    58.093333333333334,\n",
       "    59.63,\n",
       "    60.60444444444445,\n",
       "    62.088888888888896]],\n",
       "  [[1.6831595774062655,\n",
       "    1.534812954339114,\n",
       "    1.43703975206749,\n",
       "    1.3411562188782475,\n",
       "    1.2959004725244911,\n",
       "    1.2308671840212562,\n",
       "    1.213645239191299,\n",
       "    1.175760585391386,\n",
       "    1.1570994842966849,\n",
       "    1.1997008902782744,\n",
       "    1.098022661595182,\n",
       "    1.0935919133268974,\n",
       "    1.085362900979817,\n",
       "    1.1044720080257817,\n",
       "    1.063294175673615,\n",
       "    1.0520233030176975,\n",
       "    1.0666602686555549,\n",
       "    1.0643232190473513,\n",
       "    1.0333923305469481,\n",
       "    1.062043767083775],\n",
       "   [1.759747759185054,\n",
       "    1.5995576740665869,\n",
       "    1.4484002751044252,\n",
       "    1.3995646116408436,\n",
       "    1.3466140279038386,\n",
       "    1.2929379176348448,\n",
       "    1.2819524158469655,\n",
       "    1.243050025437366,\n",
       "    1.2267396768385714,\n",
       "    1.2187795843929052,\n",
       "    1.187186742032116,\n",
       "    1.193233964795416,\n",
       "    1.201187111352655,\n",
       "    1.2327559797770598,\n",
       "    1.2252960459921847,\n",
       "    1.2302448565817692,\n",
       "    1.2821213127198545,\n",
       "    1.2452972579578108,\n",
       "    1.2685587208887392,\n",
       "    1.314276822503995],\n",
       "   [2.110522580079057,\n",
       "    1.9576515787365762,\n",
       "    1.8352889848703688,\n",
       "    1.7603301854634827,\n",
       "    1.697346074845303,\n",
       "    1.6460397201166912,\n",
       "    1.60105771842328,\n",
       "    1.5493359006941319,\n",
       "    1.5295234573158352,\n",
       "    1.5035648185082457,\n",
       "    1.4799793956970626,\n",
       "    1.4755356974730438,\n",
       "    1.4697915505279193,\n",
       "    1.4626695515418595,\n",
       "    1.486758927391334,\n",
       "    1.482171139425852,\n",
       "    1.4924559552561154,\n",
       "    1.5044235546480527,\n",
       "    1.5209318423135714,\n",
       "    1.5334600266069174]],\n",
       "  [[36.736666666666665,\n",
       "    44.13777777777778,\n",
       "    47.70777777777778,\n",
       "    50.760000000000005,\n",
       "    53.961111111111116,\n",
       "    55.62222222222222,\n",
       "    56.89333333333333,\n",
       "    58.03666666666667,\n",
       "    58.30444444444445,\n",
       "    57.79222222222222,\n",
       "    60.672222222222224,\n",
       "    60.68333333333334,\n",
       "    61.23555555555556,\n",
       "    60.57888888888889,\n",
       "    62.425555555555555,\n",
       "    62.88888888888889,\n",
       "    63.027777777777786,\n",
       "    61.870000000000005,\n",
       "    63.74333333333333,\n",
       "    63.49333333333333],\n",
       "   [33.14666666666667,\n",
       "    41.08888888888889,\n",
       "    46.44555555555556,\n",
       "    48.73444444444445,\n",
       "    50.79222222222223,\n",
       "    52.80555555555555,\n",
       "    53.50111111111111,\n",
       "    55.1,\n",
       "    55.85444444444444,\n",
       "    56.41888888888889,\n",
       "    57.387777777777785,\n",
       "    57.120000000000005,\n",
       "    57.504444444444445,\n",
       "    56.489999999999995,\n",
       "    57.324444444444445,\n",
       "    57.88111111111112,\n",
       "    57.30555555555556,\n",
       "    57.89555555555555,\n",
       "    57.81111111111111,\n",
       "    57.168888888888894],\n",
       "   [22.08888888888889,\n",
       "    27.464444444444446,\n",
       "    31.327777777777776,\n",
       "    34.531111111111116,\n",
       "    36.72888888888889,\n",
       "    39.12888888888889,\n",
       "    40.69888888888889,\n",
       "    43.11,\n",
       "    44.12111111111111,\n",
       "    45.23111111111111,\n",
       "    46.068888888888885,\n",
       "    46.239999999999995,\n",
       "    46.80777777777777,\n",
       "    47.14222222222222,\n",
       "    47.3,\n",
       "    46.99111111111111,\n",
       "    47.32333333333333,\n",
       "    47.27111111111111,\n",
       "    46.931111111111115,\n",
       "    47.06444444444444]])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70cd571d-e015-4048-bdea-0d57856c7b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1.896396836435253,\n",
       "   1.5984010344201869,\n",
       "   1.4446623218669132,\n",
       "   1.3392044288868254,\n",
       "   1.2502555649896914,\n",
       "   1.2846737410873175,\n",
       "   1.1660954192788764,\n",
       "   1.1227205916392533,\n",
       "   1.0765761206434532,\n",
       "   1.0802768372337928,\n",
       "   1.0993167562410235,\n",
       "   1.0523471283641728,\n",
       "   1.0453071816062385,\n",
       "   0.9547779339111664,\n",
       "   0.9503065279938958,\n",
       "   0.9023698487911712,\n",
       "   0.8814615334638141,\n",
       "   0.9858805806460705,\n",
       "   0.8746040218763731,\n",
       "   0.8036879329857501],\n",
       "  [1.9730042593384332,\n",
       "   1.673728359863162,\n",
       "   1.5184885078871793,\n",
       "   1.428129217164083,\n",
       "   1.3223173137415538,\n",
       "   1.2527341944250194,\n",
       "   1.1928628363053908,\n",
       "   1.1457005945796317,\n",
       "   1.0867845576087183,\n",
       "   1.043328912505372,\n",
       "   0.9994076538661664,\n",
       "   0.9502070731568065,\n",
       "   1.0934111700308593,\n",
       "   0.9255639664320783,\n",
       "   0.8577949001707814,\n",
       "   0.8261074533008717,\n",
       "   0.7823491135442798,\n",
       "   0.7435439554974437,\n",
       "   0.7271724405105818,\n",
       "   0.6822288703088734],\n",
       "  [2.1999576154418965,\n",
       "   2.036997713487257,\n",
       "   1.9058921797709032,\n",
       "   1.80548307181082,\n",
       "   1.7311818911270662,\n",
       "   1.6680606175214052,\n",
       "   1.6099140315570615,\n",
       "   1.5559773363850333,\n",
       "   1.5048637412149797,\n",
       "   1.4562604356218467,\n",
       "   1.4150352652438662,\n",
       "   1.3730883474715732,\n",
       "   1.3340235518460923,\n",
       "   1.2935514483939519,\n",
       "   1.2532148046249694,\n",
       "   1.2157599876888774,\n",
       "   1.1762289912016555,\n",
       "   1.1453021862120791,\n",
       "   1.1051385964859615,\n",
       "   1.0700272382660345]],\n",
       " [[28.925555555555555,\n",
       "   41.455555555555556,\n",
       "   47.45111111111111,\n",
       "   51.36222222222222,\n",
       "   54.59444444444445,\n",
       "   53.14111111111111,\n",
       "   57.56333333333333,\n",
       "   59.52222222222222,\n",
       "   61.191111111111105,\n",
       "   61.06222222222222,\n",
       "   60.58222222222223,\n",
       "   62.28111111111111,\n",
       "   62.480000000000004,\n",
       "   65.59555555555555,\n",
       "   65.75,\n",
       "   67.68666666666667,\n",
       "   68.33222222222223,\n",
       "   64.69222222222221,\n",
       "   68.72777777777777,\n",
       "   71.33888888888889],\n",
       "  [26.24666666666667,\n",
       "   38.251111111111115,\n",
       "   44.37,\n",
       "   47.70111111111111,\n",
       "   51.83777777777778,\n",
       "   54.40555555555555,\n",
       "   56.80222222222222,\n",
       "   58.50222222222222,\n",
       "   60.77111111111111,\n",
       "   62.321111111111115,\n",
       "   64.15555555555555,\n",
       "   65.76666666666667,\n",
       "   60.523333333333326,\n",
       "   66.63,\n",
       "   69.2388888888889,\n",
       "   70.32111111111111,\n",
       "   71.81111111111112,\n",
       "   73.42,\n",
       "   73.98,\n",
       "   75.49],\n",
       "  [17.737777777777776,\n",
       "   23.263333333333332,\n",
       "   28.445555555555558,\n",
       "   32.45666666666666,\n",
       "   35.681111111111115,\n",
       "   38.205555555555556,\n",
       "   40.38888888888889,\n",
       "   42.647777777777776,\n",
       "   44.7,\n",
       "   46.58,\n",
       "   48.20888888888889,\n",
       "   49.86666666666667,\n",
       "   51.303333333333335,\n",
       "   52.99333333333334,\n",
       "   54.40888888888888,\n",
       "   55.735555555555564,\n",
       "   57.257777777777775,\n",
       "   58.20444444444445,\n",
       "   59.78333333333333,\n",
       "   61.05777777777778]],\n",
       " [[1.7582565525716,\n",
       "   1.4898140215399591,\n",
       "   1.4325477445328778,\n",
       "   1.3124373937872322,\n",
       "   1.4697639283470132,\n",
       "   1.2562347291545435,\n",
       "   1.198049275289205,\n",
       "   1.161290788684379,\n",
       "   1.1643869027664715,\n",
       "   1.130526982332495,\n",
       "   1.1271639033644036,\n",
       "   1.374859324910424,\n",
       "   1.090209410356527,\n",
       "   1.0750299643894488,\n",
       "   1.0851601476527073,\n",
       "   1.0658898859877477,\n",
       "   1.088328724049709,\n",
       "   1.0532382807948373,\n",
       "   1.0602890900759534,\n",
       "   1.0833041325042194],\n",
       "  [1.7555259080095724,\n",
       "   1.5960067566484213,\n",
       "   1.51785083132034,\n",
       "   1.4252394507215782,\n",
       "   1.3386427792297169,\n",
       "   1.271702436391603,\n",
       "   1.2775048856538804,\n",
       "   1.2268696267327124,\n",
       "   1.2205084269358353,\n",
       "   1.1889303796501323,\n",
       "   1.1997902643104845,\n",
       "   1.2657743938775226,\n",
       "   1.182824136773971,\n",
       "   1.1741877617314458,\n",
       "   1.2099740477278829,\n",
       "   1.1986372851851312,\n",
       "   1.2484496300193397,\n",
       "   1.2595862227238037,\n",
       "   1.2396938123486259,\n",
       "   1.289155465212058],\n",
       "  [2.141269875351678,\n",
       "   1.9807509801943193,\n",
       "   1.8660445430062034,\n",
       "   1.7901280930435115,\n",
       "   1.7345891266383908,\n",
       "   1.6738081132485108,\n",
       "   1.6150269911370494,\n",
       "   1.5674923409453847,\n",
       "   1.5362024769525637,\n",
       "   1.5192668786780401,\n",
       "   1.4879595994610677,\n",
       "   1.4851904225281694,\n",
       "   1.4619058448482642,\n",
       "   1.4570862439207055,\n",
       "   1.4565034569664435,\n",
       "   1.4618274176662618,\n",
       "   1.4885634752837094,\n",
       "   1.4765080951831557,\n",
       "   1.4974810539490797,\n",
       "   1.529768471267413]],\n",
       " [[36.44,\n",
       "   45.37,\n",
       "   47.84777777777778,\n",
       "   52.65555555555556,\n",
       "   47.09777777777778,\n",
       "   55.04555555555556,\n",
       "   56.718888888888884,\n",
       "   58.01444444444444,\n",
       "   58.06888888888889,\n",
       "   59.63222222222222,\n",
       "   59.648888888888884,\n",
       "   50.81888888888889,\n",
       "   60.72333333333333,\n",
       "   61.35444444444445,\n",
       "   61.10666666666666,\n",
       "   62.05888888888889,\n",
       "   61.00444444444444,\n",
       "   62.794444444444444,\n",
       "   62.641111111111115,\n",
       "   62.28555555555556],\n",
       "  [34.95444444444444,\n",
       "   41.44555555555556,\n",
       "   44.83111111111111,\n",
       "   48.06,\n",
       "   51.46222222222222,\n",
       "   53.855555555555554,\n",
       "   53.684444444444445,\n",
       "   55.571111111111115,\n",
       "   56.13555555555556,\n",
       "   57.36444444444444,\n",
       "   57.36222222222223,\n",
       "   55.05444444444444,\n",
       "   57.668888888888894,\n",
       "   58.56888888888889,\n",
       "   57.90555555555555,\n",
       "   58.44888888888888,\n",
       "   57.51777777777778,\n",
       "   57.77777777777777,\n",
       "   58.08333333333333,\n",
       "   57.99333333333333],\n",
       "  [21.83222222222222,\n",
       "   27.392222222222223,\n",
       "   30.88888888888889,\n",
       "   34.583333333333336,\n",
       "   36.28666666666667,\n",
       "   38.71666666666667,\n",
       "   40.39111111111111,\n",
       "   42.60777777777778,\n",
       "   43.766666666666666,\n",
       "   44.74,\n",
       "   45.57,\n",
       "   45.56111111111111,\n",
       "   46.81111111111111,\n",
       "   47.034444444444446,\n",
       "   47.31666666666667,\n",
       "   47.49666666666666,\n",
       "   47.04333333333333,\n",
       "   47.46888888888889,\n",
       "   47.55666666666667,\n",
       "   47.07222222222222]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07487f3d-7e8b-45ab-96b6-e290f201394b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
