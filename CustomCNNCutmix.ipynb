{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dd2297d-cb00-4200-8c1c-f4976dd032fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ba1c5f-5ce5-429f-a7bd-467aece5e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_set = ImageFolder(root='cinic10/versions/1/train//', transform=transform)\n",
    "validate_set = ImageFolder(root='cinic10/versions/1/valid//', transform=transform)\n",
    "test_set = ImageFolder(root='cinic10/versions/1/test//', transform=transform)\n",
    "\n",
    "class_names = train_set.classes\n",
    "num_labels = len(class_names)\n",
    "\n",
    "def prepare_transforms():\n",
    "    \n",
    "    train_set = ImageFolder(root='cinic10/versions/1/train//', transform=transform_default)\n",
    "    validate_set = ImageFolder(root='cinic10/versions/1/valid//', transform=transform_default)\n",
    "    test_set = ImageFolder(root='cinic10/versions/1/test//', transform=transform_default)\n",
    "        \n",
    "    data_loader = DataLoader(train_set, batch_size=64, num_workers=6, generator=torch.Generator(device='cpu'),pin_memory=True, shuffle=True,persistent_workers=True, prefetch_factor=4)\n",
    "    data_loader_val = DataLoader(validate_set, batch_size=64, num_workers=3, generator=torch.Generator(device='cpu'),pin_memory=True, shuffle=True,persistent_workers=True, prefetch_factor=4)\n",
    "    data_loader_test = DataLoader(test_set, batch_size=64, num_workers=3, generator=torch.Generator(device='cpu'),persistent_workers=True)\n",
    "\n",
    "    return data_loader, data_loader_val, data_loader_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68d0ec3c-2e25-4c64-b68e-38930cce112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_2_dropout(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=10):\n",
    "        super(CNN_2_dropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=4, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.linear1 = nn.Linear(64 * 3 * 3  , 100)\n",
    "        self.linear2 = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)  # Startujemy od wymiaru 1, aby nie spłaszczać batch_size\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cd553a3-dc19-449c-81f2-2df0a3d4bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "cutmix = v2.CutMix(num_classes=num_labels)\n",
    "\n",
    "def prepare_model_resnet_cutmix(lr):\n",
    "    model = CNN_2_dropout()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, optimizer ,criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e800a942-ef5d-443f-999a-7ec27acc6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,data_loader_test, criterion,device):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (batch_X, batch_Y) in enumerate(data_loader_test):\n",
    "        X = batch_X.to(device, non_blocking=True)\n",
    "        Y = batch_Y.to(device, non_blocking=True)\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == Y).sum().item()\n",
    "        total += Y.size(0)\n",
    "    avg_test_loss = test_loss / len(data_loader_test)\n",
    "    test_accuracy = correct / total * 100\n",
    "    print(f\" Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")            \n",
    "    return  avg_test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1952a17-e52e-4c49-899b-daf906fd373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_cutmix(lr, model, seed):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    cutmix = v2.CutMix(num_classes=num_labels)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    \n",
    "    losses_tr = []\n",
    "    accuracies_tr = []\n",
    "    losses_val = []\n",
    "    accuracies_val = []\n",
    "    data_loader, data_loader_val, data_loader_test = prepare_transforms()\n",
    "            \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "        \n",
    "    num_epochs = 60\n",
    "    \n",
    "    prev_prev_prev_loss = float('inf')\n",
    "    prev_prev_loss = float('inf')\n",
    "    prev_loss = float('inf')\n",
    "    curr_loss = float('inf')\n",
    "\n",
    "    model = model()\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs): \n",
    "        print('Entered the loop')\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "                \n",
    "        for i, (batch_X, batch_Y) in enumerate(data_loader):\n",
    "            batch_X, batch_Y = cutmix(batch_X, batch_Y)\n",
    "            X = batch_X.to(device, non_blocking=True)  \n",
    "            Y = batch_Y.to(device, non_blocking=True)  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs,Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "    \n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_X, batch_Y) in enumerate(data_loader_val):\n",
    "                if i >= 140:  \n",
    "                    break\n",
    "                X = batch_X.to(device, non_blocking=True)\n",
    "                Y = batch_Y.to(device, non_blocking=True)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, Y)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == Y).sum().item()\n",
    "                total += Y.size(0)\n",
    "        avg_val_loss = val_loss / (140)\n",
    "        val_accuracy = correct / total * 100\n",
    "        print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")            \n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "    \n",
    "        if(avg_val_loss>curr_loss and curr_loss>prev_loss and prev_loss > prev_prev_loss and prev_prev_loss> prev_prev_prev_loss):\n",
    "            losses_tr.append(train_losses)\n",
    "            accuracies_tr.append(train_accuracies) \n",
    "            losses_val.append(val_losses)  \n",
    "            accuracies_val.append(val_accuracies)\n",
    "            avg_test_loss, test_accuracy = test(model, data_loader_test, criterion, device)\n",
    "            break\n",
    "        prev_prev_prev_loss = prev_prev_loss  \n",
    "        prev_prev_loss = prev_loss\n",
    "        prev_loss = curr_loss\n",
    "        curr_loss = avg_val_loss\n",
    "    \n",
    "        if(epoch == num_epochs - 1 ):\n",
    "            losses_tr.append(train_losses)\n",
    "            accuracies_tr.append(train_accuracies) \n",
    "            losses_val.append(val_losses)  \n",
    "            accuracies_val.append(val_accuracies) \n",
    "            avg_test_loss, test_accuracy = test(model, data_loader_test, criterion, device)\n",
    "\n",
    "    return model,lr,val_losses, val_accuracies, train_losses, avg_test_loss, test_accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c527393c-4139-465e-8e55-0e1699d5c556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered the loop\n",
      "Epoch 1, Average Loss: 2.0562\n",
      "Epoch 1, Validation Loss: 1.6784, Accuracy: 39.07%\n",
      "Entered the loop\n",
      "Epoch 2, Average Loss: 1.9551\n",
      "Epoch 2, Validation Loss: 1.5940, Accuracy: 42.04%\n",
      "Entered the loop\n",
      "Epoch 3, Average Loss: 1.9065\n",
      "Epoch 3, Validation Loss: 1.5309, Accuracy: 44.45%\n",
      "Entered the loop\n",
      "Epoch 4, Average Loss: 1.8731\n",
      "Epoch 4, Validation Loss: 1.4815, Accuracy: 46.13%\n",
      "Entered the loop\n",
      "Epoch 5, Average Loss: 1.8421\n",
      "Epoch 5, Validation Loss: 1.4077, Accuracy: 51.62%\n",
      "Entered the loop\n",
      "Epoch 6, Average Loss: 1.8217\n",
      "Epoch 6, Validation Loss: 1.3508, Accuracy: 53.11%\n",
      "Entered the loop\n",
      "Epoch 7, Average Loss: 1.8062\n",
      "Epoch 7, Validation Loss: 1.3476, Accuracy: 53.50%\n",
      "Entered the loop\n",
      "Epoch 8, Average Loss: 1.7956\n",
      "Epoch 8, Validation Loss: 1.3787, Accuracy: 52.33%\n",
      "Entered the loop\n",
      "Epoch 9, Average Loss: 1.7815\n",
      "Epoch 9, Validation Loss: 1.3139, Accuracy: 53.64%\n",
      "Entered the loop\n",
      "Epoch 10, Average Loss: 1.7815\n",
      "Epoch 10, Validation Loss: 1.2910, Accuracy: 55.30%\n",
      "Entered the loop\n",
      "Epoch 11, Average Loss: 1.7710\n",
      "Epoch 11, Validation Loss: 1.3156, Accuracy: 54.70%\n",
      "Entered the loop\n",
      "Epoch 12, Average Loss: 1.7572\n",
      "Epoch 12, Validation Loss: 1.2551, Accuracy: 57.27%\n",
      "Entered the loop\n",
      "Epoch 13, Average Loss: 1.7520\n",
      "Epoch 13, Validation Loss: 1.2482, Accuracy: 57.27%\n",
      "Entered the loop\n",
      "Epoch 14, Average Loss: 1.7483\n",
      "Epoch 14, Validation Loss: 1.2835, Accuracy: 55.64%\n",
      "Entered the loop\n",
      "Epoch 15, Average Loss: 1.7397\n",
      "Epoch 15, Validation Loss: 1.2527, Accuracy: 56.15%\n",
      "Entered the loop\n",
      "Epoch 16, Average Loss: 1.7407\n",
      "Epoch 16, Validation Loss: 1.2304, Accuracy: 57.85%\n",
      "Entered the loop\n",
      "Epoch 17, Average Loss: 1.7299\n",
      "Epoch 17, Validation Loss: 1.2629, Accuracy: 55.57%\n",
      "Entered the loop\n",
      "Epoch 18, Average Loss: 1.7333\n",
      "Epoch 18, Validation Loss: 1.2487, Accuracy: 57.37%\n",
      "Entered the loop\n",
      "Epoch 19, Average Loss: 1.7353\n",
      "Epoch 19, Validation Loss: 1.2490, Accuracy: 58.00%\n",
      "Entered the loop\n",
      "Epoch 20, Average Loss: 1.7320\n",
      "Epoch 20, Validation Loss: 1.2356, Accuracy: 58.01%\n",
      "Entered the loop\n",
      "Epoch 21, Average Loss: 1.7187\n",
      "Epoch 21, Validation Loss: 1.2280, Accuracy: 58.84%\n",
      "Entered the loop\n",
      "Epoch 22, Average Loss: 1.7087\n",
      "Epoch 22, Validation Loss: 1.2089, Accuracy: 57.72%\n",
      "Entered the loop\n",
      "Epoch 23, Average Loss: 1.7124\n",
      "Epoch 23, Validation Loss: 1.2530, Accuracy: 57.10%\n",
      "Entered the loop\n",
      "Epoch 24, Average Loss: 1.7150\n",
      "Epoch 24, Validation Loss: 1.2303, Accuracy: 58.02%\n",
      "Entered the loop\n",
      "Epoch 25, Average Loss: 1.7103\n",
      "Epoch 25, Validation Loss: 1.2209, Accuracy: 57.51%\n",
      "Entered the loop\n",
      "Epoch 26, Average Loss: 1.7092\n",
      "Epoch 26, Validation Loss: 1.2070, Accuracy: 59.25%\n",
      "Entered the loop\n",
      "Epoch 27, Average Loss: 1.7057\n",
      "Epoch 27, Validation Loss: 1.1901, Accuracy: 58.77%\n",
      "Entered the loop\n",
      "Epoch 28, Average Loss: 1.7050\n",
      "Epoch 28, Validation Loss: 1.1710, Accuracy: 59.80%\n",
      "Entered the loop\n",
      "Epoch 29, Average Loss: 1.7055\n",
      "Epoch 29, Validation Loss: 1.2207, Accuracy: 57.54%\n",
      "Entered the loop\n",
      "Epoch 30, Average Loss: 1.7012\n",
      "Epoch 30, Validation Loss: 1.1800, Accuracy: 60.37%\n",
      "Entered the loop\n",
      "Epoch 31, Average Loss: 1.7008\n",
      "Epoch 31, Validation Loss: 1.2030, Accuracy: 58.78%\n",
      "Entered the loop\n",
      "Epoch 32, Average Loss: 1.6945\n",
      "Epoch 32, Validation Loss: 1.2142, Accuracy: 58.78%\n",
      "Entered the loop\n",
      "Epoch 33, Average Loss: 1.6945\n",
      "Epoch 33, Validation Loss: 1.1374, Accuracy: 60.32%\n",
      "Entered the loop\n",
      "Epoch 34, Average Loss: 1.6933\n",
      "Epoch 34, Validation Loss: 1.1654, Accuracy: 59.93%\n",
      "Entered the loop\n",
      "Epoch 35, Average Loss: 1.6942\n",
      "Epoch 35, Validation Loss: 1.1579, Accuracy: 60.75%\n",
      "Entered the loop\n",
      "Epoch 36, Average Loss: 1.6956\n",
      "Epoch 36, Validation Loss: 1.1738, Accuracy: 60.22%\n",
      "Entered the loop\n",
      "Epoch 37, Average Loss: 1.6908\n",
      "Epoch 37, Validation Loss: 1.1963, Accuracy: 59.07%\n",
      "Entered the loop\n",
      "Epoch 38, Average Loss: 1.6933\n",
      "Epoch 38, Validation Loss: 1.1787, Accuracy: 59.96%\n",
      "Entered the loop\n",
      "Epoch 39, Average Loss: 1.6933\n",
      "Epoch 39, Validation Loss: 1.2146, Accuracy: 58.06%\n",
      "Entered the loop\n",
      "Epoch 40, Average Loss: 1.6837\n",
      "Epoch 40, Validation Loss: 1.1696, Accuracy: 60.01%\n",
      "Entered the loop\n",
      "Epoch 41, Average Loss: 1.6845\n",
      "Epoch 41, Validation Loss: 1.1731, Accuracy: 60.32%\n",
      "Entered the loop\n",
      "Epoch 42, Average Loss: 1.6850\n",
      "Epoch 42, Validation Loss: 1.1549, Accuracy: 60.69%\n",
      "Entered the loop\n",
      "Epoch 43, Average Loss: 1.6671\n",
      "Epoch 43, Validation Loss: 1.1628, Accuracy: 60.40%\n",
      "Entered the loop\n",
      "Epoch 44, Average Loss: 1.6816\n",
      "Epoch 44, Validation Loss: 1.1389, Accuracy: 61.14%\n",
      "Entered the loop\n",
      "Epoch 45, Average Loss: 1.6774\n",
      "Epoch 45, Validation Loss: 1.1499, Accuracy: 60.56%\n",
      "Entered the loop\n",
      "Epoch 46, Average Loss: 1.6821\n",
      "Epoch 46, Validation Loss: 1.1601, Accuracy: 60.01%\n",
      "Entered the loop\n",
      "Epoch 47, Average Loss: 1.6805\n",
      "Epoch 47, Validation Loss: 1.1476, Accuracy: 60.04%\n",
      "Entered the loop\n",
      "Epoch 48, Average Loss: 1.6752\n",
      "Epoch 48, Validation Loss: 1.1507, Accuracy: 60.83%\n",
      "Entered the loop\n",
      "Epoch 49, Average Loss: 1.6705\n",
      "Epoch 49, Validation Loss: 1.1515, Accuracy: 61.29%\n",
      "Entered the loop\n",
      "Epoch 50, Average Loss: 1.6704\n",
      "Epoch 50, Validation Loss: 1.1554, Accuracy: 61.36%\n",
      "Entered the loop\n",
      "Epoch 51, Average Loss: 1.6736\n",
      "Epoch 51, Validation Loss: 1.1641, Accuracy: 59.40%\n",
      " Test Loss: 1.1574, Test Accuracy: 59.84%\n",
      "Entered the loop\n",
      "Epoch 1, Average Loss: 2.1579\n",
      "Epoch 1, Validation Loss: 1.8962, Accuracy: 30.06%\n",
      "Entered the loop\n",
      "Epoch 2, Average Loss: 2.0624\n",
      "Epoch 2, Validation Loss: 1.8267, Accuracy: 33.08%\n",
      "Entered the loop\n",
      "Epoch 3, Average Loss: 2.0288\n",
      "Epoch 3, Validation Loss: 1.7909, Accuracy: 34.49%\n",
      "Entered the loop\n",
      "Epoch 4, Average Loss: 2.0110\n",
      "Epoch 4, Validation Loss: 1.7493, Accuracy: 36.65%\n",
      "Entered the loop\n",
      "Epoch 5, Average Loss: 1.9936\n",
      "Epoch 5, Validation Loss: 1.7170, Accuracy: 37.54%\n",
      "Entered the loop\n",
      "Epoch 6, Average Loss: 1.9827\n",
      "Epoch 6, Validation Loss: 1.7111, Accuracy: 39.00%\n",
      "Entered the loop\n",
      "Epoch 7, Average Loss: 1.9700\n",
      "Epoch 7, Validation Loss: 1.6691, Accuracy: 40.64%\n",
      "Entered the loop\n",
      "Epoch 8, Average Loss: 1.9583\n",
      "Epoch 8, Validation Loss: 1.6765, Accuracy: 40.30%\n",
      "Entered the loop\n",
      "Epoch 9, Average Loss: 1.9481\n",
      "Epoch 9, Validation Loss: 1.6292, Accuracy: 41.85%\n",
      "Entered the loop\n",
      "Epoch 10, Average Loss: 1.9417\n",
      "Epoch 10, Validation Loss: 1.6165, Accuracy: 42.17%\n",
      "Entered the loop\n",
      "Epoch 11, Average Loss: 1.9294\n",
      "Epoch 11, Validation Loss: 1.6020, Accuracy: 43.60%\n",
      "Entered the loop\n",
      "Epoch 12, Average Loss: 1.9155\n",
      "Epoch 12, Validation Loss: 1.5680, Accuracy: 44.45%\n",
      "Entered the loop\n",
      "Epoch 13, Average Loss: 1.9062\n",
      "Epoch 13, Validation Loss: 1.5686, Accuracy: 44.21%\n",
      "Entered the loop\n",
      "Epoch 14, Average Loss: 1.9026\n",
      "Epoch 14, Validation Loss: 1.5635, Accuracy: 44.73%\n",
      "Entered the loop\n",
      "Epoch 15, Average Loss: 1.8907\n",
      "Epoch 15, Validation Loss: 1.5455, Accuracy: 45.23%\n",
      "Entered the loop\n",
      "Epoch 16, Average Loss: 1.8876\n",
      "Epoch 16, Validation Loss: 1.5035, Accuracy: 46.88%\n",
      "Entered the loop\n",
      "Epoch 17, Average Loss: 1.8794\n",
      "Epoch 17, Validation Loss: 1.5160, Accuracy: 46.99%\n",
      "Entered the loop\n",
      "Epoch 18, Average Loss: 1.8781\n",
      "Epoch 18, Validation Loss: 1.4998, Accuracy: 47.50%\n",
      "Entered the loop\n",
      "Epoch 19, Average Loss: 1.8765\n",
      "Epoch 19, Validation Loss: 1.5062, Accuracy: 46.17%\n",
      "Entered the loop\n",
      "Epoch 20, Average Loss: 1.8706\n",
      "Epoch 20, Validation Loss: 1.4733, Accuracy: 48.26%\n",
      "Entered the loop\n",
      "Epoch 21, Average Loss: 1.8599\n",
      "Epoch 21, Validation Loss: 1.4820, Accuracy: 47.79%\n",
      "Entered the loop\n",
      "Epoch 22, Average Loss: 1.8483\n",
      "Epoch 22, Validation Loss: 1.4688, Accuracy: 48.55%\n",
      "Entered the loop\n",
      "Epoch 23, Average Loss: 1.8483\n",
      "Epoch 23, Validation Loss: 1.4650, Accuracy: 48.73%\n",
      "Entered the loop\n",
      "Epoch 24, Average Loss: 1.8476\n",
      "Epoch 24, Validation Loss: 1.4492, Accuracy: 49.13%\n",
      "Entered the loop\n",
      "Epoch 25, Average Loss: 1.8421\n",
      "Epoch 25, Validation Loss: 1.4555, Accuracy: 48.95%\n",
      "Entered the loop\n",
      "Epoch 26, Average Loss: 1.8380\n",
      "Epoch 26, Validation Loss: 1.4461, Accuracy: 50.13%\n",
      "Entered the loop\n",
      "Epoch 27, Average Loss: 1.8333\n",
      "Epoch 27, Validation Loss: 1.4173, Accuracy: 51.17%\n",
      "Entered the loop\n",
      "Epoch 28, Average Loss: 1.8294\n",
      "Epoch 28, Validation Loss: 1.4093, Accuracy: 50.55%\n",
      "Entered the loop\n",
      "Epoch 29, Average Loss: 1.8282\n",
      "Epoch 29, Validation Loss: 1.4039, Accuracy: 51.58%\n",
      "Entered the loop\n",
      "Epoch 30, Average Loss: 1.8224\n",
      "Epoch 30, Validation Loss: 1.3907, Accuracy: 52.04%\n",
      "Entered the loop\n",
      "Epoch 31, Average Loss: 1.8175\n",
      "Epoch 31, Validation Loss: 1.3974, Accuracy: 51.92%\n",
      "Entered the loop\n",
      "Epoch 32, Average Loss: 1.8147\n",
      "Epoch 32, Validation Loss: 1.4035, Accuracy: 51.66%\n",
      "Entered the loop\n",
      "Epoch 33, Average Loss: 1.8086\n",
      "Epoch 33, Validation Loss: 1.3719, Accuracy: 52.32%\n",
      "Entered the loop\n",
      "Epoch 34, Average Loss: 1.8043\n",
      "Epoch 34, Validation Loss: 1.3680, Accuracy: 52.59%\n",
      "Entered the loop\n",
      "Epoch 35, Average Loss: 1.8050\n",
      "Epoch 35, Validation Loss: 1.3625, Accuracy: 52.82%\n",
      "Entered the loop\n",
      "Epoch 36, Average Loss: 1.8039\n",
      "Epoch 36, Validation Loss: 1.3613, Accuracy: 52.42%\n",
      "Entered the loop\n",
      "Epoch 37, Average Loss: 1.7982\n",
      "Epoch 37, Validation Loss: 1.3744, Accuracy: 52.12%\n",
      "Entered the loop\n",
      "Epoch 38, Average Loss: 1.7959\n",
      "Epoch 38, Validation Loss: 1.3633, Accuracy: 53.31%\n",
      "Entered the loop\n",
      "Epoch 39, Average Loss: 1.7977\n",
      "Epoch 39, Validation Loss: 1.3902, Accuracy: 51.69%\n",
      "Entered the loop\n",
      "Epoch 40, Average Loss: 1.7846\n",
      "Epoch 40, Validation Loss: 1.3502, Accuracy: 54.12%\n",
      "Entered the loop\n",
      "Epoch 41, Average Loss: 1.7846\n",
      "Epoch 41, Validation Loss: 1.3529, Accuracy: 53.24%\n",
      "Entered the loop\n",
      "Epoch 42, Average Loss: 1.7844\n",
      "Epoch 42, Validation Loss: 1.3314, Accuracy: 54.73%\n",
      "Entered the loop\n",
      "Epoch 43, Average Loss: 1.7647\n",
      "Epoch 43, Validation Loss: 1.3240, Accuracy: 54.55%\n",
      "Entered the loop\n",
      "Epoch 44, Average Loss: 1.7775\n",
      "Epoch 44, Validation Loss: 1.3099, Accuracy: 55.22%\n",
      "Entered the loop\n",
      "Epoch 45, Average Loss: 1.7721\n",
      "Epoch 45, Validation Loss: 1.3034, Accuracy: 54.85%\n",
      "Entered the loop\n",
      "Epoch 46, Average Loss: 1.7729\n",
      "Epoch 46, Validation Loss: 1.3254, Accuracy: 54.50%\n",
      "Entered the loop\n",
      "Epoch 47, Average Loss: 1.7731\n",
      "Epoch 47, Validation Loss: 1.3175, Accuracy: 54.07%\n",
      "Entered the loop\n",
      "Epoch 48, Average Loss: 1.7655\n",
      "Epoch 48, Validation Loss: 1.3051, Accuracy: 54.64%\n",
      "Entered the loop\n",
      "Epoch 49, Average Loss: 1.7610\n",
      "Epoch 49, Validation Loss: 1.3073, Accuracy: 55.04%\n",
      "Entered the loop\n",
      "Epoch 50, Average Loss: 1.7611\n",
      "Epoch 50, Validation Loss: 1.3358, Accuracy: 54.31%\n",
      "Entered the loop\n",
      "Epoch 51, Average Loss: 1.7613\n",
      "Epoch 51, Validation Loss: 1.2984, Accuracy: 54.32%\n",
      "Entered the loop\n",
      "Epoch 52, Average Loss: 1.7534\n",
      "Epoch 52, Validation Loss: 1.2952, Accuracy: 54.02%\n",
      "Entered the loop\n",
      "Epoch 53, Average Loss: 1.7592\n",
      "Epoch 53, Validation Loss: 1.2744, Accuracy: 56.26%\n",
      "Entered the loop\n",
      "Epoch 54, Average Loss: 1.7563\n",
      "Epoch 54, Validation Loss: 1.3153, Accuracy: 54.13%\n",
      "Entered the loop\n",
      "Epoch 55, Average Loss: 1.7499\n",
      "Epoch 55, Validation Loss: 1.3005, Accuracy: 55.56%\n",
      "Entered the loop\n",
      "Epoch 56, Average Loss: 1.7632\n",
      "Epoch 56, Validation Loss: 1.2802, Accuracy: 56.34%\n",
      "Entered the loop\n",
      "Epoch 57, Average Loss: 1.7452\n",
      "Epoch 57, Validation Loss: 1.2647, Accuracy: 55.61%\n",
      "Entered the loop\n",
      "Epoch 58, Average Loss: 1.7565\n",
      "Epoch 58, Validation Loss: 1.2738, Accuracy: 56.84%\n",
      "Entered the loop\n",
      "Epoch 59, Average Loss: 1.7450\n",
      "Epoch 59, Validation Loss: 1.2597, Accuracy: 57.35%\n",
      "Entered the loop\n",
      "Epoch 60, Average Loss: 1.7435\n",
      "Epoch 60, Validation Loss: 1.2636, Accuracy: 56.98%\n",
      " Test Loss: 1.2745, Test Accuracy: 56.44%\n"
     ]
    }
   ],
   "source": [
    "lrates = [0.001,0.0001]\n",
    "results = []\n",
    "for lr in lrates:\n",
    "    results.append(model_train_cutmix(lr,CNN_2_dropout,42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73f06820-2680-4667-ab00-e247d81e6421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(CNN_2_dropout(\n",
       "    (conv1): Conv2d(3, 16, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (linear1): Linear(in_features=576, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  ),\n",
       "  0.001,\n",
       "  [1.6784299084118435,\n",
       "   1.5940250354153769,\n",
       "   1.530903683389936,\n",
       "   1.4815372722489493,\n",
       "   1.4076869479247502,\n",
       "   1.35075124672481,\n",
       "   1.34758323601314,\n",
       "   1.3786517117704664,\n",
       "   1.3139239404882703,\n",
       "   1.2910399300711495,\n",
       "   1.315556012732642,\n",
       "   1.2550739169120788,\n",
       "   1.2481779562575477,\n",
       "   1.283465439081192,\n",
       "   1.252739006280899,\n",
       "   1.2303933722632272,\n",
       "   1.2628968570913588,\n",
       "   1.2486748473984854,\n",
       "   1.2489891179970332,\n",
       "   1.2356467034135545,\n",
       "   1.2279863161700113,\n",
       "   1.2088507494756153,\n",
       "   1.2530273846217563,\n",
       "   1.230258703657559,\n",
       "   1.2209163819040572,\n",
       "   1.2069915499005999,\n",
       "   1.1900899478367397,\n",
       "   1.1709749894482748,\n",
       "   1.2206744853939329,\n",
       "   1.1800000705889293,\n",
       "   1.2030402941363199,\n",
       "   1.2142126057829177,\n",
       "   1.137358991588865,\n",
       "   1.1653943393911634,\n",
       "   1.1578595293419702,\n",
       "   1.1737725675106048,\n",
       "   1.1962518061910357,\n",
       "   1.178653295976775,\n",
       "   1.2146175018378667,\n",
       "   1.1696138939687184,\n",
       "   1.173088492665972,\n",
       "   1.1548610840524947,\n",
       "   1.1627670641456331,\n",
       "   1.1389067769050598,\n",
       "   1.1498574163232531,\n",
       "   1.1601302014929908,\n",
       "   1.1475720307656696,\n",
       "   1.1507425010204315,\n",
       "   1.1514648258686067,\n",
       "   1.1554490532193864,\n",
       "   1.1641494346516472],\n",
       "  [39.073660714285715,\n",
       "   42.042410714285715,\n",
       "   44.453125,\n",
       "   46.127232142857146,\n",
       "   51.61830357142857,\n",
       "   53.113839285714285,\n",
       "   53.504464285714285,\n",
       "   52.332589285714285,\n",
       "   53.63839285714286,\n",
       "   55.30133928571429,\n",
       "   54.698660714285715,\n",
       "   57.265625,\n",
       "   57.265625,\n",
       "   55.63616071428571,\n",
       "   56.14955357142857,\n",
       "   57.845982142857146,\n",
       "   55.56919642857143,\n",
       "   57.36607142857143,\n",
       "   58.002232142857146,\n",
       "   58.013392857142854,\n",
       "   58.839285714285715,\n",
       "   57.723214285714285,\n",
       "   57.09821428571429,\n",
       "   58.02455357142857,\n",
       "   57.511160714285715,\n",
       "   59.25223214285714,\n",
       "   58.77232142857143,\n",
       "   59.799107142857146,\n",
       "   57.54464285714286,\n",
       "   60.36830357142857,\n",
       "   58.783482142857146,\n",
       "   58.783482142857146,\n",
       "   60.32366071428571,\n",
       "   59.93303571428571,\n",
       "   60.747767857142854,\n",
       "   60.22321428571429,\n",
       "   59.073660714285715,\n",
       "   59.955357142857146,\n",
       "   58.058035714285715,\n",
       "   60.011160714285715,\n",
       "   60.32366071428571,\n",
       "   60.691964285714285,\n",
       "   60.401785714285715,\n",
       "   61.138392857142854,\n",
       "   60.558035714285715,\n",
       "   60.011160714285715,\n",
       "   60.04464285714286,\n",
       "   60.82589285714286,\n",
       "   61.294642857142854,\n",
       "   61.361607142857146,\n",
       "   59.39732142857142],\n",
       "  [2.0561935491568666,\n",
       "   1.9551485443928602,\n",
       "   1.9065383063535277,\n",
       "   1.8730688117968757,\n",
       "   1.8421103340052678,\n",
       "   1.8216538369952149,\n",
       "   1.8061511993577655,\n",
       "   1.795555403322384,\n",
       "   1.7814656903579376,\n",
       "   1.781492256237076,\n",
       "   1.7710033606872884,\n",
       "   1.7571694267160434,\n",
       "   1.7520146861344088,\n",
       "   1.7483481742341043,\n",
       "   1.7396710855196618,\n",
       "   1.7407084545008071,\n",
       "   1.7299421143684306,\n",
       "   1.7332821647241425,\n",
       "   1.7353199733091569,\n",
       "   1.7319670856295533,\n",
       "   1.718662738122242,\n",
       "   1.7087236976657425,\n",
       "   1.7124484835911407,\n",
       "   1.7149809949687804,\n",
       "   1.7103379676760557,\n",
       "   1.7092425641051063,\n",
       "   1.7056991727367392,\n",
       "   1.705040719971728,\n",
       "   1.705507639459286,\n",
       "   1.701240206493413,\n",
       "   1.7007815150733356,\n",
       "   1.6945298990206932,\n",
       "   1.6944874410690276,\n",
       "   1.6932743784800097,\n",
       "   1.6942335902670214,\n",
       "   1.6956412281308855,\n",
       "   1.6908281822956956,\n",
       "   1.6933287544799511,\n",
       "   1.693308406165922,\n",
       "   1.6836740016598353,\n",
       "   1.684485130599821,\n",
       "   1.6849620693019713,\n",
       "   1.667077570751193,\n",
       "   1.6816481409466004,\n",
       "   1.6773853195670996,\n",
       "   1.6820920998523146,\n",
       "   1.6804715917732331,\n",
       "   1.6752322837233797,\n",
       "   1.670450620051386,\n",
       "   1.670403882003288,\n",
       "   1.6735981040671943],\n",
       "  1.1574491770367463,\n",
       "  59.83555555555555),\n",
       " (CNN_2_dropout(\n",
       "    (conv1): Conv2d(3, 16, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (linear1): Linear(in_features=576, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  ),\n",
       "  0.0001,\n",
       "  [1.8961710844721114,\n",
       "   1.826685677255903,\n",
       "   1.7909422286919185,\n",
       "   1.7493042605263847,\n",
       "   1.717018619605473,\n",
       "   1.7111457671437944,\n",
       "   1.6690651076180594,\n",
       "   1.676525374821254,\n",
       "   1.629226562806538,\n",
       "   1.6164588144847325,\n",
       "   1.601974277836936,\n",
       "   1.5679504590375082,\n",
       "   1.5685767327036177,\n",
       "   1.563534449679511,\n",
       "   1.5454623333045414,\n",
       "   1.5035314440727234,\n",
       "   1.5160090020724706,\n",
       "   1.4997886231967381,\n",
       "   1.506169957774026,\n",
       "   1.4732839030878884,\n",
       "   1.481999625478472,\n",
       "   1.4688069437231337,\n",
       "   1.4650038191250392,\n",
       "   1.4491791827338083,\n",
       "   1.4554894711290087,\n",
       "   1.4461244523525238,\n",
       "   1.4172743627003261,\n",
       "   1.4093094646930695,\n",
       "   1.4039110950061253,\n",
       "   1.3906567828995842,\n",
       "   1.3974084598677499,\n",
       "   1.4035317437989372,\n",
       "   1.3719115461621965,\n",
       "   1.367996335029602,\n",
       "   1.362460617508207,\n",
       "   1.3612900240080696,\n",
       "   1.3743950349943979,\n",
       "   1.363253412076405,\n",
       "   1.3902076533862522,\n",
       "   1.3502245758261,\n",
       "   1.3529128994260515,\n",
       "   1.3314211249351502,\n",
       "   1.3239618514265332,\n",
       "   1.3099472676004682,\n",
       "   1.303364532334464,\n",
       "   1.3254381682191576,\n",
       "   1.317495176621846,\n",
       "   1.3051093322890146,\n",
       "   1.3072858316557747,\n",
       "   1.3358462486948286,\n",
       "   1.2983638456889561,\n",
       "   1.2951758444309234,\n",
       "   1.2743950690541948,\n",
       "   1.3152777118342263,\n",
       "   1.3004565932921002,\n",
       "   1.2801650549684251,\n",
       "   1.2647288126604899,\n",
       "   1.2738435293946948,\n",
       "   1.2597311862877436,\n",
       "   1.2635730990341731],\n",
       "  [30.05580357142857,\n",
       "   33.08035714285714,\n",
       "   34.486607142857146,\n",
       "   36.65178571428571,\n",
       "   37.544642857142854,\n",
       "   38.99553571428571,\n",
       "   40.636160714285715,\n",
       "   40.30133928571429,\n",
       "   41.85267857142857,\n",
       "   42.16517857142857,\n",
       "   43.604910714285715,\n",
       "   44.453125,\n",
       "   44.20758928571429,\n",
       "   44.73214285714286,\n",
       "   45.234375,\n",
       "   46.875,\n",
       "   46.986607142857146,\n",
       "   47.5,\n",
       "   46.171875,\n",
       "   48.25892857142857,\n",
       "   47.79017857142857,\n",
       "   48.549107142857146,\n",
       "   48.72767857142858,\n",
       "   49.129464285714285,\n",
       "   48.95089285714286,\n",
       "   50.13392857142858,\n",
       "   51.171875,\n",
       "   50.546875,\n",
       "   51.58482142857142,\n",
       "   52.042410714285715,\n",
       "   51.919642857142854,\n",
       "   51.66294642857143,\n",
       "   52.32142857142858,\n",
       "   52.589285714285715,\n",
       "   52.823660714285715,\n",
       "   52.42187499999999,\n",
       "   52.12053571428571,\n",
       "   53.314732142857146,\n",
       "   51.68526785714286,\n",
       "   54.11830357142857,\n",
       "   53.23660714285714,\n",
       "   54.73214285714286,\n",
       "   54.55357142857142,\n",
       "   55.223214285714285,\n",
       "   54.85491071428571,\n",
       "   54.497767857142854,\n",
       "   54.07366071428571,\n",
       "   54.64285714285714,\n",
       "   55.044642857142854,\n",
       "   54.308035714285715,\n",
       "   54.31919642857142,\n",
       "   54.01785714285714,\n",
       "   56.261160714285715,\n",
       "   54.12946428571429,\n",
       "   55.558035714285715,\n",
       "   56.339285714285715,\n",
       "   55.613839285714285,\n",
       "   56.841517857142854,\n",
       "   57.35491071428571,\n",
       "   56.97544642857143],\n",
       "  [2.1578973735912355,\n",
       "   2.0623583924965225,\n",
       "   2.0288375239890777,\n",
       "   2.011044432274738,\n",
       "   1.9936418674817387,\n",
       "   1.9826991188670187,\n",
       "   1.969950012137217,\n",
       "   1.9582956601477042,\n",
       "   1.9480720223817913,\n",
       "   1.9416525742646729,\n",
       "   1.9293862399092445,\n",
       "   1.91550156048366,\n",
       "   1.9061926329652723,\n",
       "   1.9025691424012607,\n",
       "   1.8906650542365633,\n",
       "   1.8875863758824611,\n",
       "   1.8793820973059432,\n",
       "   1.878110429710247,\n",
       "   1.8765280347558397,\n",
       "   1.870568365938882,\n",
       "   1.859940750597213,\n",
       "   1.8482519364475611,\n",
       "   1.848323446177555,\n",
       "   1.8476275217473803,\n",
       "   1.8420899529446926,\n",
       "   1.8380206890282431,\n",
       "   1.833251219737928,\n",
       "   1.8293746130298705,\n",
       "   1.8281979756568796,\n",
       "   1.8224208698936957,\n",
       "   1.8174943393566292,\n",
       "   1.8146960097335294,\n",
       "   1.808590710459657,\n",
       "   1.8043263340640254,\n",
       "   1.8050249926825321,\n",
       "   1.8038903911993194,\n",
       "   1.7982136521173353,\n",
       "   1.7958909388374757,\n",
       "   1.7977375741875925,\n",
       "   1.7845945526033575,\n",
       "   1.7846430029187883,\n",
       "   1.7844310621886534,\n",
       "   1.7647140469208793,\n",
       "   1.7775350634562013,\n",
       "   1.7720687059790172,\n",
       "   1.7729287610972444,\n",
       "   1.773059385417621,\n",
       "   1.7654609104039911,\n",
       "   1.7610314044023798,\n",
       "   1.7610763392299307,\n",
       "   1.7612811759082494,\n",
       "   1.7533517379496397,\n",
       "   1.7591552159933648,\n",
       "   1.7562877730265865,\n",
       "   1.749944086268004,\n",
       "   1.7632418598955883,\n",
       "   1.7451757330714852,\n",
       "   1.7564701348564349,\n",
       "   1.7450208418235427,\n",
       "   1.743521604371901],\n",
       "  1.2745345893826312,\n",
       "  56.44444444444444)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3171a0-83c0-4312-864f-3b385ffd59ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
