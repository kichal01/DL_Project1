{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a267ce-8548-4311-b136-beefce3a31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77de9905-afeb-4717-bb8d-d75f25025107",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_default = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "def prepare_transforms():\n",
    "    \n",
    "    train_set = ImageFolder(root='cinic10/versions/1/train//', transform=transform_default)\n",
    "    validate_set = ImageFolder(root='cinic10/versions/1/valid//', transform=transform_default)\n",
    "    test_set = ImageFolder(root='cinic10/versions/1/test//', transform=transform_default)\n",
    "        \n",
    "    data_loader = DataLoader(train_set, batch_size=64, num_workers=6, generator=torch.Generator(device='cpu'),pin_memory=True, shuffle=True,persistent_workers=True, prefetch_factor=4)\n",
    "    data_loader_val = DataLoader(validate_set, batch_size=128, num_workers=2, generator=torch.Generator(device='cpu'),pin_memory=True, shuffle=True,persistent_workers=True, prefetch_factor=4)\n",
    "    data_loader_test = DataLoader(test_set, batch_size=128, num_workers=2, generator=torch.Generator(device='cpu'),persistent_workers=True)\n",
    "\n",
    "    return data_loader, data_loader_val, data_loader_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e05fa7-a641-41a5-8fff-14e52b154f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef664734-db1c-4c28-a4ab-8f13ad3cb19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=4, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.linear1 = nn.Linear(3600  , 200)\n",
    "        self.linear2 = nn.Linear(200, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.flatten(x, 1)  \n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ce0ffd-ddd0-4f80-906a-6f50e91bad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_2(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=10):\n",
    "        super(CNN_2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=4, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.linear1 = nn.Linear(32 * 7 * 7  , 100)\n",
    "        self.linear2 = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, 1)  \n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad53bc3-1386-416c-99c3-e35bccbfd6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_dropout(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=10):\n",
    "        super(CNN_dropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=4, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.linear1 = nn.Linear(32 * 7 * 7  , 100)\n",
    "        self.linear2 = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, 1)  \n",
    "        x = self.dropout(x)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d2ea64f-d36f-4356-8376-1dd5d7862dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_2_dropout(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=10):\n",
    "        super(CNN_2_dropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=4, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.linear1 = nn.Linear(64 * 3 * 3  , 100)\n",
    "        self.linear2 = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)  \n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e83872-315b-4911-935e-7bc1322ffb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e90a24b-d807-4f54-97a6-abf1557229f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_set[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "train_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9bde51-7f25-4e14-b2d8-76cc120b465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.Tensor(1,train_set[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0570f2-41cf-4a80-893d-60626355eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_set = ImageFolder(root='cinic10/versions/1/train//', transform=transform)\n",
    "validate_set = ImageFolder(root='cinic10/versions/1/valid//', transform=transform)\n",
    "test_set = ImageFolder(root='cinic10/versions/1/test//', transform=transform)\n",
    "\n",
    "class_names = train_set.classes\n",
    "num_labels = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae9752-ee4b-4e63-816e-56f1a334a619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc3644d-9b79-4ff5-8db1-ceb03d89d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [0.001,0.0001]\n",
    "models= [CNN, CNN_2,CNN_dropout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75d885d2-7c0e-4787-8225-6824c78c512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,data_loader_test, criterion,device):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (batch_X, batch_Y) in enumerate(data_loader_test):\n",
    "        X = batch_X.to(device, non_blocking=True)\n",
    "        Y = batch_Y.to(device, non_blocking=True)\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == Y).sum().item()\n",
    "        total += Y.size(0)\n",
    "    avg_test_loss = test_loss / len(data_loader_test)\n",
    "    test_accuracy = correct / total * 100\n",
    "    print(f\" Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")            \n",
    "    return  avg_test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f511697-cdd3-48d0-b39b-d941d0fd4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(lr, model, seed):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    \n",
    "    losses_tr = []\n",
    "    accuracies_tr = []\n",
    "    losses_val = []\n",
    "    accuracies_val = []\n",
    "    data_loader, data_loader_val, data_loader_test = prepare_transforms()\n",
    "            \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "        \n",
    "    num_epochs = 60\n",
    "    \n",
    "    prev_prev_prev_loss = float('inf')\n",
    "    prev_prev_loss = float('inf')\n",
    "    prev_loss = float('inf')\n",
    "    curr_loss = float('inf')\n",
    "\n",
    "    model = model()\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs): \n",
    "        print('Entered the loop')\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "                \n",
    "        for i, (batch_X, batch_Y) in enumerate(data_loader):\n",
    "            X = batch_X.to(device, non_blocking=True)  \n",
    "            Y = batch_Y.to(device, non_blocking=True)  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == Y).sum().item()\n",
    "            total += Y.size(0)\n",
    "                \n",
    "    \n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        train_accuracy = correct / total * 100\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_X, batch_Y) in enumerate(data_loader_val):\n",
    "                if i >= 140:  \n",
    "                    break\n",
    "                X = batch_X.to(device, non_blocking=True)\n",
    "                Y = batch_Y.to(device, non_blocking=True)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, Y)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == Y).sum().item()\n",
    "                total += Y.size(0)\n",
    "        avg_val_loss = val_loss / (140)\n",
    "        val_accuracy = correct / total * 100\n",
    "        print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")            \n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "    \n",
    "        if(avg_val_loss>curr_loss and curr_loss>prev_loss and prev_loss > prev_prev_loss and prev_prev_loss> prev_prev_prev_loss):\n",
    "            losses_tr.append(train_losses)\n",
    "            accuracies_tr.append(train_accuracies) \n",
    "            losses_val.append(val_losses)  \n",
    "            accuracies_val.append(val_accuracies)\n",
    "            avg_test_loss, test_accuracy = test(model, data_loader_test, criterion, device)\n",
    "            break\n",
    "        prev_prev_prev_loss = prev_prev_loss  \n",
    "        prev_prev_loss = prev_loss\n",
    "        prev_loss = curr_loss\n",
    "        curr_loss = avg_val_loss\n",
    "    \n",
    "        if(epoch == num_epochs - 1 ):\n",
    "            losses_tr.append(train_losses)\n",
    "            accuracies_tr.append(train_accuracies) \n",
    "            losses_val.append(val_losses)  \n",
    "            accuracies_val.append(val_accuracies) \n",
    "            avg_test_loss, test_accuracy = test(model, data_loader_test, criterion, device)\n",
    "\n",
    "    return model,lr,val_losses, val_accuracies, train_losses, train_accuracies, avg_test_loss, test_accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be0bf386-2c24-4c8d-a587-1ae72cf92eb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered the loop\n",
      "Epoch 1, Average Loss: 1.6929\n",
      "Epoch 1, Validation Loss: 1.5190, Accuracy: 45.08%\n",
      "Entered the loop\n",
      "Epoch 2, Average Loss: 1.4734\n",
      "Epoch 2, Validation Loss: 1.4487, Accuracy: 46.84%\n",
      "Entered the loop\n",
      "Epoch 3, Average Loss: 1.3690\n",
      "Epoch 3, Validation Loss: 1.3462, Accuracy: 50.77%\n",
      "Entered the loop\n",
      "Epoch 4, Average Loss: 1.2982\n",
      "Epoch 4, Validation Loss: 1.3095, Accuracy: 52.44%\n",
      "Entered the loop\n",
      "Epoch 5, Average Loss: 1.2497\n",
      "Epoch 5, Validation Loss: 1.2450, Accuracy: 54.93%\n",
      "Entered the loop\n",
      "Epoch 6, Average Loss: 1.2131\n",
      "Epoch 6, Validation Loss: 1.2134, Accuracy: 55.96%\n",
      "Entered the loop\n",
      "Epoch 7, Average Loss: 1.1783\n",
      "Epoch 7, Validation Loss: 1.1922, Accuracy: 57.25%\n",
      "Entered the loop\n",
      "Epoch 8, Average Loss: 1.1558\n",
      "Epoch 8, Validation Loss: 1.2116, Accuracy: 56.14%\n",
      "Entered the loop\n",
      "Epoch 9, Average Loss: 1.1271\n",
      "Epoch 9, Validation Loss: 1.1874, Accuracy: 57.57%\n",
      "Entered the loop\n",
      "Epoch 10, Average Loss: 1.1077\n",
      "Epoch 10, Validation Loss: 1.1515, Accuracy: 58.38%\n",
      "Entered the loop\n",
      "Epoch 11, Average Loss: 1.0912\n",
      "Epoch 11, Validation Loss: 1.1933, Accuracy: 57.04%\n",
      "Entered the loop\n",
      "Epoch 12, Average Loss: 1.0721\n",
      "Epoch 12, Validation Loss: 1.1571, Accuracy: 58.11%\n",
      "Entered the loop\n",
      "Epoch 13, Average Loss: 1.0568\n",
      "Epoch 13, Validation Loss: 1.1564, Accuracy: 58.42%\n",
      "Entered the loop\n",
      "Epoch 14, Average Loss: 1.0398\n",
      "Epoch 14, Validation Loss: 1.2162, Accuracy: 56.46%\n",
      "Entered the loop\n",
      "Epoch 15, Average Loss: 1.0286\n",
      "Epoch 15, Validation Loss: 1.1870, Accuracy: 57.47%\n",
      "Entered the loop\n",
      "Epoch 16, Average Loss: 1.0152\n",
      "Epoch 16, Validation Loss: 1.1766, Accuracy: 58.73%\n",
      "Entered the loop\n",
      "Epoch 17, Average Loss: 1.0052\n",
      "Epoch 17, Validation Loss: 1.1866, Accuracy: 57.33%\n",
      "Entered the loop\n",
      "Epoch 18, Average Loss: 0.9952\n",
      "Epoch 18, Validation Loss: 1.1479, Accuracy: 59.12%\n",
      "Entered the loop\n",
      "Epoch 19, Average Loss: 0.9858\n",
      "Epoch 19, Validation Loss: 1.1696, Accuracy: 58.11%\n",
      "Entered the loop\n",
      "Epoch 20, Average Loss: 0.9723\n",
      "Epoch 20, Validation Loss: 1.1565, Accuracy: 58.96%\n",
      "Entered the loop\n",
      "Epoch 21, Average Loss: 0.9619\n",
      "Epoch 21, Validation Loss: 1.1480, Accuracy: 59.53%\n",
      "Entered the loop\n",
      "Epoch 22, Average Loss: 0.9532\n",
      "Epoch 22, Validation Loss: 1.1707, Accuracy: 58.78%\n",
      "Entered the loop\n",
      "Epoch 23, Average Loss: 0.9449\n",
      "Epoch 23, Validation Loss: 1.1855, Accuracy: 58.38%\n",
      "Entered the loop\n",
      "Epoch 24, Average Loss: 0.9389\n",
      "Epoch 24, Validation Loss: 1.1487, Accuracy: 58.59%\n",
      "Entered the loop\n",
      "Epoch 25, Average Loss: 0.9324\n",
      "Epoch 25, Validation Loss: 1.1917, Accuracy: 58.33%\n",
      "Entered the loop\n",
      "Epoch 26, Average Loss: 0.9219\n",
      "Epoch 26, Validation Loss: 1.2108, Accuracy: 58.26%\n",
      "Entered the loop\n",
      "Epoch 27, Average Loss: 0.9195\n",
      "Epoch 27, Validation Loss: 1.1815, Accuracy: 59.45%\n",
      "Entered the loop\n",
      "Epoch 28, Average Loss: 0.9146\n",
      "Epoch 28, Validation Loss: 1.1745, Accuracy: 58.59%\n",
      "Entered the loop\n",
      "Epoch 29, Average Loss: 0.9050\n",
      "Epoch 29, Validation Loss: 1.1834, Accuracy: 58.05%\n",
      "Entered the loop\n",
      "Epoch 30, Average Loss: 0.9003\n",
      "Epoch 30, Validation Loss: 1.1803, Accuracy: 58.56%\n",
      "Entered the loop\n",
      "Epoch 31, Average Loss: 0.8973\n",
      "Epoch 31, Validation Loss: 1.2034, Accuracy: 57.75%\n",
      "Entered the loop\n",
      "Epoch 32, Average Loss: 0.8866\n",
      "Epoch 32, Validation Loss: 1.2141, Accuracy: 58.07%\n",
      "Entered the loop\n",
      "Epoch 33, Average Loss: 0.8826\n",
      "Epoch 33, Validation Loss: 1.1949, Accuracy: 57.99%\n",
      "Entered the loop\n",
      "Epoch 34, Average Loss: 0.8786\n",
      "Epoch 34, Validation Loss: 1.2371, Accuracy: 56.83%\n",
      "Entered the loop\n",
      "Epoch 35, Average Loss: 0.8779\n",
      "Epoch 35, Validation Loss: 1.2152, Accuracy: 57.76%\n",
      "Entered the loop\n",
      "Epoch 36, Average Loss: 0.8704\n",
      "Epoch 36, Validation Loss: 1.1959, Accuracy: 58.87%\n",
      "Entered the loop\n",
      "Epoch 37, Average Loss: 0.8691\n",
      "Epoch 37, Validation Loss: 1.2428, Accuracy: 57.34%\n",
      "Entered the loop\n",
      "Epoch 38, Average Loss: 0.8635\n",
      "Epoch 38, Validation Loss: 1.2169, Accuracy: 57.87%\n",
      "Entered the loop\n",
      "Epoch 39, Average Loss: 0.8557\n",
      "Epoch 39, Validation Loss: 1.2418, Accuracy: 57.96%\n",
      "Entered the loop\n",
      "Epoch 40, Average Loss: 0.8506\n",
      "Epoch 40, Validation Loss: 1.2058, Accuracy: 58.14%\n",
      "Entered the loop\n",
      "Epoch 41, Average Loss: 0.8496\n",
      "Epoch 41, Validation Loss: 1.2554, Accuracy: 58.10%\n",
      "Entered the loop\n",
      "Epoch 42, Average Loss: 0.8458\n",
      "Epoch 42, Validation Loss: 1.2293, Accuracy: 57.60%\n",
      "Entered the loop\n",
      "Epoch 43, Average Loss: 0.8440\n",
      "Epoch 43, Validation Loss: 1.2207, Accuracy: 58.56%\n",
      "Entered the loop\n",
      "Epoch 44, Average Loss: 0.8374\n",
      "Epoch 44, Validation Loss: 1.2313, Accuracy: 58.23%\n",
      "Entered the loop\n",
      "Epoch 45, Average Loss: 0.8430\n",
      "Epoch 45, Validation Loss: 1.2036, Accuracy: 59.00%\n",
      "Entered the loop\n",
      "Epoch 46, Average Loss: 0.8340\n",
      "Epoch 46, Validation Loss: 1.2453, Accuracy: 57.52%\n",
      "Entered the loop\n",
      "Epoch 47, Average Loss: 0.8282\n",
      "Epoch 47, Validation Loss: 1.2560, Accuracy: 57.42%\n",
      "Entered the loop\n",
      "Epoch 48, Average Loss: 0.8285\n",
      "Epoch 48, Validation Loss: 1.2584, Accuracy: 57.89%\n",
      "Entered the loop\n",
      "Epoch 49, Average Loss: 0.8287\n",
      "Epoch 49, Validation Loss: 1.2644, Accuracy: 58.40%\n",
      " Test Loss: 1.2690, Test ccuracy: 58.03%\n",
      "Entered the loop\n",
      "Epoch 1, Average Loss: 1.6768\n",
      "Epoch 1, Validation Loss: 1.5021, Accuracy: 45.08%\n",
      "Entered the loop\n",
      "Epoch 2, Average Loss: 1.4306\n",
      "Epoch 2, Validation Loss: 1.3964, Accuracy: 49.01%\n",
      "Entered the loop\n",
      "Epoch 3, Average Loss: 1.3224\n",
      "Epoch 3, Validation Loss: 1.3072, Accuracy: 52.87%\n",
      "Entered the loop\n",
      "Epoch 4, Average Loss: 1.2509\n",
      "Epoch 4, Validation Loss: 1.2884, Accuracy: 53.20%\n",
      "Entered the loop\n",
      "Epoch 5, Average Loss: 1.1975\n",
      "Epoch 5, Validation Loss: 1.2668, Accuracy: 54.41%\n",
      "Entered the loop\n",
      "Epoch 6, Average Loss: 1.1578\n",
      "Epoch 6, Validation Loss: 1.2349, Accuracy: 55.52%\n",
      "Entered the loop\n",
      "Epoch 7, Average Loss: 1.1229\n",
      "Epoch 7, Validation Loss: 1.2057, Accuracy: 57.14%\n",
      "Entered the loop\n",
      "Epoch 8, Average Loss: 1.0944\n",
      "Epoch 8, Validation Loss: 1.2507, Accuracy: 55.64%\n",
      "Entered the loop\n",
      "Epoch 9, Average Loss: 1.0670\n",
      "Epoch 9, Validation Loss: 1.1858, Accuracy: 57.67%\n",
      "Entered the loop\n",
      "Epoch 10, Average Loss: 1.0399\n",
      "Epoch 10, Validation Loss: 1.2064, Accuracy: 56.51%\n",
      "Entered the loop\n",
      "Epoch 11, Average Loss: 1.0193\n",
      "Epoch 11, Validation Loss: 1.2648, Accuracy: 54.84%\n",
      "Entered the loop\n",
      "Epoch 12, Average Loss: 0.9986\n",
      "Epoch 12, Validation Loss: 1.2240, Accuracy: 57.78%\n",
      "Entered the loop\n",
      "Epoch 13, Average Loss: 0.9788\n",
      "Epoch 13, Validation Loss: 1.2429, Accuracy: 57.11%\n",
      "Entered the loop\n",
      "Epoch 14, Average Loss: 0.9613\n",
      "Epoch 14, Validation Loss: 1.2662, Accuracy: 55.52%\n",
      "Entered the loop\n",
      "Epoch 15, Average Loss: 0.9413\n",
      "Epoch 15, Validation Loss: 1.2941, Accuracy: 54.16%\n",
      "Entered the loop\n",
      "Epoch 16, Average Loss: 0.9264\n",
      "Epoch 16, Validation Loss: 1.2776, Accuracy: 56.69%\n",
      "Entered the loop\n",
      "Epoch 17, Average Loss: 0.9081\n",
      "Epoch 17, Validation Loss: 1.2885, Accuracy: 55.60%\n",
      "Entered the loop\n",
      "Epoch 18, Average Loss: 0.8909\n",
      "Epoch 18, Validation Loss: 1.2877, Accuracy: 56.15%\n",
      "Entered the loop\n",
      "Epoch 19, Average Loss: 0.8770\n",
      "Epoch 19, Validation Loss: 1.2764, Accuracy: 56.46%\n",
      "Entered the loop\n",
      "Epoch 20, Average Loss: 0.8614\n",
      "Epoch 20, Validation Loss: 1.3110, Accuracy: 56.21%\n",
      "Entered the loop\n",
      "Epoch 21, Average Loss: 0.8441\n",
      "Epoch 21, Validation Loss: 1.2956, Accuracy: 56.40%\n",
      "Entered the loop\n",
      "Epoch 22, Average Loss: 0.8309\n",
      "Epoch 22, Validation Loss: 1.3317, Accuracy: 55.79%\n",
      "Entered the loop\n",
      "Epoch 23, Average Loss: 0.8159\n",
      "Epoch 23, Validation Loss: 1.3887, Accuracy: 55.11%\n",
      "Entered the loop\n",
      "Epoch 24, Average Loss: 0.8038\n",
      "Epoch 24, Validation Loss: 1.3324, Accuracy: 55.45%\n",
      "Entered the loop\n",
      "Epoch 25, Average Loss: 0.7863\n",
      "Epoch 25, Validation Loss: 1.4688, Accuracy: 55.02%\n",
      "Entered the loop\n",
      "Epoch 26, Average Loss: 0.7752\n",
      "Epoch 26, Validation Loss: 1.4157, Accuracy: 56.16%\n",
      "Entered the loop\n",
      "Epoch 27, Average Loss: 0.7595\n",
      "Epoch 27, Validation Loss: 1.4492, Accuracy: 55.31%\n",
      "Entered the loop\n",
      "Epoch 28, Average Loss: 0.7490\n",
      "Epoch 28, Validation Loss: 1.4510, Accuracy: 55.93%\n",
      "Entered the loop\n",
      "Epoch 29, Average Loss: 0.7371\n",
      "Epoch 29, Validation Loss: 1.4929, Accuracy: 54.92%\n",
      "Entered the loop\n",
      "Epoch 30, Average Loss: 0.7223\n",
      "Epoch 30, Validation Loss: 1.5170, Accuracy: 54.93%\n",
      " Test Loss: 1.5167, Test ccuracy: 54.57%\n",
      "Entered the loop\n",
      "Epoch 1, Average Loss: 1.6183\n",
      "Epoch 1, Validation Loss: 1.4766, Accuracy: 46.94%\n",
      "Entered the loop\n",
      "Epoch 2, Average Loss: 1.4109\n",
      "Epoch 2, Validation Loss: 1.4135, Accuracy: 48.17%\n",
      "Entered the loop\n",
      "Epoch 3, Average Loss: 1.3117\n",
      "Epoch 3, Validation Loss: 1.3615, Accuracy: 50.29%\n",
      "Entered the loop\n",
      "Epoch 4, Average Loss: 1.2275\n",
      "Epoch 4, Validation Loss: 1.3401, Accuracy: 51.44%\n",
      "Entered the loop\n",
      "Epoch 5, Average Loss: 1.1560\n",
      "Epoch 5, Validation Loss: 1.2997, Accuracy: 53.49%\n",
      "Entered the loop\n",
      "Epoch 6, Average Loss: 1.0944\n",
      "Epoch 6, Validation Loss: 1.3600, Accuracy: 52.51%\n",
      "Entered the loop\n",
      "Epoch 7, Average Loss: 1.0356\n",
      "Epoch 7, Validation Loss: 1.3006, Accuracy: 54.43%\n",
      "Entered the loop\n",
      "Epoch 8, Average Loss: 0.9822\n",
      "Epoch 8, Validation Loss: 1.3776, Accuracy: 52.79%\n",
      "Entered the loop\n",
      "Epoch 9, Average Loss: 0.9219\n",
      "Epoch 9, Validation Loss: 1.3716, Accuracy: 53.08%\n",
      "Entered the loop\n",
      "Epoch 10, Average Loss: 0.8703\n",
      "Epoch 10, Validation Loss: 1.4568, Accuracy: 51.60%\n",
      "Entered the loop\n",
      "Epoch 11, Average Loss: 0.8194\n",
      "Epoch 11, Validation Loss: 1.4640, Accuracy: 53.21%\n",
      "Entered the loop\n",
      "Epoch 12, Average Loss: 0.7695\n",
      "Epoch 12, Validation Loss: 1.4859, Accuracy: 53.34%\n",
      "Entered the loop\n",
      "Epoch 13, Average Loss: 0.7187\n",
      "Epoch 13, Validation Loss: 1.5561, Accuracy: 52.78%\n",
      " Test Loss: 1.5697, Test ccuracy: 52.81%\n",
      "Entered the loop\n",
      "Epoch 1, Average Loss: 1.9341\n",
      "Epoch 1, Validation Loss: 1.7740, Accuracy: 35.69%\n",
      "Entered the loop\n",
      "Epoch 2, Average Loss: 1.7239\n",
      "Epoch 2, Validation Loss: 1.6758, Accuracy: 38.95%\n",
      "Entered the loop\n",
      "Epoch 3, Average Loss: 1.6520\n",
      "Epoch 3, Validation Loss: 1.6373, Accuracy: 40.08%\n",
      "Entered the loop\n",
      "Epoch 4, Average Loss: 1.6122\n",
      "Epoch 4, Validation Loss: 1.5969, Accuracy: 41.32%\n",
      "Entered the loop\n",
      "Epoch 5, Average Loss: 1.5813\n",
      "Epoch 5, Validation Loss: 1.5653, Accuracy: 42.85%\n",
      "Entered the loop\n",
      "Epoch 6, Average Loss: 1.5570\n",
      "Epoch 6, Validation Loss: 1.5369, Accuracy: 44.80%\n",
      "Entered the loop\n",
      "Epoch 7, Average Loss: 1.5345\n",
      "Epoch 7, Validation Loss: 1.5002, Accuracy: 45.47%\n",
      "Entered the loop\n",
      "Epoch 8, Average Loss: 1.5137\n",
      "Epoch 8, Validation Loss: 1.5143, Accuracy: 45.19%\n",
      "Entered the loop\n",
      "Epoch 9, Average Loss: 1.4943\n",
      "Epoch 9, Validation Loss: 1.4771, Accuracy: 46.48%\n",
      "Entered the loop\n",
      "Epoch 10, Average Loss: 1.4760\n",
      "Epoch 10, Validation Loss: 1.4501, Accuracy: 47.63%\n",
      "Entered the loop\n",
      "Epoch 11, Average Loss: 1.4585\n",
      "Epoch 11, Validation Loss: 1.4474, Accuracy: 47.92%\n",
      "Entered the loop\n",
      "Epoch 12, Average Loss: 1.4421\n",
      "Epoch 12, Validation Loss: 1.4205, Accuracy: 48.65%\n",
      "Entered the loop\n",
      "Epoch 13, Average Loss: 1.4258\n",
      "Epoch 13, Validation Loss: 1.4066, Accuracy: 49.21%\n",
      "Entered the loop\n",
      "Epoch 14, Average Loss: 1.4115\n",
      "Epoch 14, Validation Loss: 1.4177, Accuracy: 49.31%\n",
      "Entered the loop\n",
      "Epoch 15, Average Loss: 1.3953\n",
      "Epoch 15, Validation Loss: 1.3904, Accuracy: 49.27%\n",
      "Entered the loop\n",
      "Epoch 16, Average Loss: 1.3820\n",
      "Epoch 16, Validation Loss: 1.3471, Accuracy: 51.93%\n",
      "Entered the loop\n",
      "Epoch 17, Average Loss: 1.3683\n",
      "Epoch 17, Validation Loss: 1.3728, Accuracy: 50.00%\n",
      "Entered the loop\n",
      "Epoch 18, Average Loss: 1.3538\n",
      "Epoch 18, Validation Loss: 1.3416, Accuracy: 52.08%\n",
      "Entered the loop\n",
      "Epoch 19, Average Loss: 1.3421\n",
      "Epoch 19, Validation Loss: 1.3478, Accuracy: 51.07%\n",
      "Entered the loop\n",
      "Epoch 20, Average Loss: 1.3322\n",
      "Epoch 20, Validation Loss: 1.3109, Accuracy: 53.10%\n",
      "Entered the loop\n",
      "Epoch 21, Average Loss: 1.3220\n",
      "Epoch 21, Validation Loss: 1.3120, Accuracy: 52.81%\n",
      "Entered the loop\n",
      "Epoch 22, Average Loss: 1.3137\n",
      "Epoch 22, Validation Loss: 1.3135, Accuracy: 52.81%\n",
      "Entered the loop\n",
      "Epoch 23, Average Loss: 1.3018\n",
      "Epoch 23, Validation Loss: 1.3174, Accuracy: 52.68%\n",
      "Entered the loop\n",
      "Epoch 24, Average Loss: 1.2909\n",
      "Epoch 24, Validation Loss: 1.2868, Accuracy: 53.76%\n",
      "Entered the loop\n",
      "Epoch 25, Average Loss: 1.2835\n",
      "Epoch 25, Validation Loss: 1.2864, Accuracy: 54.17%\n",
      "Entered the loop\n",
      "Epoch 26, Average Loss: 1.2738\n",
      "Epoch 26, Validation Loss: 1.2919, Accuracy: 53.43%\n",
      "Entered the loop\n",
      "Epoch 27, Average Loss: 1.2674\n",
      "Epoch 27, Validation Loss: 1.2848, Accuracy: 53.72%\n",
      "Entered the loop\n",
      "Epoch 28, Average Loss: 1.2581\n",
      "Epoch 28, Validation Loss: 1.2599, Accuracy: 54.94%\n",
      "Entered the loop\n",
      "Epoch 29, Average Loss: 1.2521\n",
      "Epoch 29, Validation Loss: 1.2722, Accuracy: 53.64%\n",
      "Entered the loop\n",
      "Epoch 30, Average Loss: 1.2441\n",
      "Epoch 30, Validation Loss: 1.2639, Accuracy: 55.41%\n",
      "Entered the loop\n",
      "Epoch 31, Average Loss: 1.2369\n",
      "Epoch 31, Validation Loss: 1.2553, Accuracy: 54.64%\n",
      "Entered the loop\n",
      "Epoch 32, Average Loss: 1.2302\n",
      "Epoch 32, Validation Loss: 1.2869, Accuracy: 53.76%\n",
      "Entered the loop\n",
      "Epoch 33, Average Loss: 1.2235\n",
      "Epoch 33, Validation Loss: 1.2332, Accuracy: 55.40%\n",
      "Entered the loop\n",
      "Epoch 34, Average Loss: 1.2175\n",
      "Epoch 34, Validation Loss: 1.2879, Accuracy: 53.65%\n",
      "Entered the loop\n",
      "Epoch 35, Average Loss: 1.2107\n",
      "Epoch 35, Validation Loss: 1.2367, Accuracy: 55.56%\n",
      "Entered the loop\n",
      "Epoch 36, Average Loss: 1.2048\n",
      "Epoch 36, Validation Loss: 1.2232, Accuracy: 55.75%\n",
      "Entered the loop\n",
      "Epoch 37, Average Loss: 1.1972\n",
      "Epoch 37, Validation Loss: 1.2533, Accuracy: 54.87%\n",
      "Entered the loop\n",
      "Epoch 38, Average Loss: 1.1914\n",
      "Epoch 38, Validation Loss: 1.2277, Accuracy: 56.16%\n",
      "Entered the loop\n",
      "Epoch 39, Average Loss: 1.1857\n",
      "Epoch 39, Validation Loss: 1.2461, Accuracy: 55.19%\n",
      "Entered the loop\n",
      "Epoch 40, Average Loss: 1.1770\n",
      "Epoch 40, Validation Loss: 1.2172, Accuracy: 55.84%\n",
      "Entered the loop\n",
      "Epoch 41, Average Loss: 1.1717\n",
      "Epoch 41, Validation Loss: 1.2201, Accuracy: 56.38%\n",
      "Entered the loop\n",
      "Epoch 42, Average Loss: 1.1675\n",
      "Epoch 42, Validation Loss: 1.2144, Accuracy: 56.92%\n",
      "Entered the loop\n",
      "Epoch 43, Average Loss: 1.1602\n",
      "Epoch 43, Validation Loss: 1.2011, Accuracy: 56.83%\n",
      "Entered the loop\n",
      "Epoch 44, Average Loss: 1.1536\n",
      "Epoch 44, Validation Loss: 1.1930, Accuracy: 57.39%\n",
      "Entered the loop\n",
      "Epoch 45, Average Loss: 1.1488\n",
      "Epoch 45, Validation Loss: 1.1940, Accuracy: 56.86%\n",
      "Entered the loop\n",
      "Epoch 46, Average Loss: 1.1448\n",
      "Epoch 46, Validation Loss: 1.2163, Accuracy: 56.26%\n",
      "Entered the loop\n",
      "Epoch 47, Average Loss: 1.1378\n",
      "Epoch 47, Validation Loss: 1.2240, Accuracy: 56.18%\n",
      "Entered the loop\n",
      "Epoch 48, Average Loss: 1.1338\n",
      "Epoch 48, Validation Loss: 1.1999, Accuracy: 56.17%\n",
      "Entered the loop\n",
      "Epoch 49, Average Loss: 1.1272\n",
      "Epoch 49, Validation Loss: 1.1967, Accuracy: 57.34%\n",
      "Entered the loop\n",
      "Epoch 50, Average Loss: 1.1220\n",
      "Epoch 50, Validation Loss: 1.2286, Accuracy: 56.22%\n",
      "Entered the loop\n",
      "Epoch 51, Average Loss: 1.1179\n",
      "Epoch 51, Validation Loss: 1.1946, Accuracy: 56.34%\n",
      "Entered the loop\n",
      "Epoch 52, Average Loss: 1.1118\n",
      "Epoch 52, Validation Loss: 1.1788, Accuracy: 58.01%\n",
      "Entered the loop\n",
      "Epoch 53, Average Loss: 1.1093\n",
      "Epoch 53, Validation Loss: 1.1970, Accuracy: 57.24%\n",
      "Entered the loop\n",
      "Epoch 54, Average Loss: 1.0992\n",
      "Epoch 54, Validation Loss: 1.2044, Accuracy: 57.13%\n",
      "Entered the loop\n",
      "Epoch 55, Average Loss: 1.0984\n",
      "Epoch 55, Validation Loss: 1.1905, Accuracy: 57.37%\n",
      "Entered the loop\n",
      "Epoch 56, Average Loss: 1.0941\n",
      "Epoch 56, Validation Loss: 1.1858, Accuracy: 57.23%\n",
      "Entered the loop\n",
      "Epoch 57, Average Loss: 1.0852\n",
      "Epoch 57, Validation Loss: 1.1853, Accuracy: 57.50%\n",
      "Entered the loop\n",
      "Epoch 58, Average Loss: 1.0809\n",
      "Epoch 58, Validation Loss: 1.1746, Accuracy: 58.27%\n",
      "Entered the loop\n",
      "Epoch 59, Average Loss: 1.0797\n",
      "Epoch 59, Validation Loss: 1.1659, Accuracy: 58.45%\n",
      "Entered the loop\n",
      "Epoch 60, Average Loss: 1.0734\n",
      "Epoch 60, Validation Loss: 1.1750, Accuracy: 57.62%\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'avg_test_loss' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m lrate:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:       \n\u001b[1;32m----> 6\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(model_train(lr,model,\u001b[38;5;241m42\u001b[39m))\n",
      "Cell \u001b[1;32mIn[9], line 103\u001b[0m, in \u001b[0;36mmodel_train\u001b[1;34m(lr, model, seed)\u001b[0m\n\u001b[0;32m    100\u001b[0m         losses_val\u001b[38;5;241m.\u001b[39mappend(val_losses)  \n\u001b[0;32m    101\u001b[0m         accuracies_val\u001b[38;5;241m.\u001b[39mappend(val_accuracies) \n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model,lr,val_losses, val_accuracies, train_losses, train_accuracies, avg_test_loss, test_accuracy\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'avg_test_loss' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "lrate = [0.001,0.0001]\n",
    "models= [CNN_dropout, CNN_2,CNN]\n",
    "results = []\n",
    "for lr in lrate:\n",
    "    for model in models:       \n",
    "        results.append(model_train(lr,model,42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66aa6320-1f17-4274-bb46-95db4b2cbf29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(CNN_dropout(\n",
       "    (conv1): Conv2d(3, 16, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (linear1): Linear(in_features=1568, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  ),\n",
       "  0.001,\n",
       "  [1.518992062977382,\n",
       "   1.4486844965389796,\n",
       "   1.3462432503700257,\n",
       "   1.3094680522169386,\n",
       "   1.2450068478073393,\n",
       "   1.2133797986166819,\n",
       "   1.1922112528766904,\n",
       "   1.2115562877484731,\n",
       "   1.1873556396790914,\n",
       "   1.1514819170747483,\n",
       "   1.193324555669512,\n",
       "   1.1570603102445602,\n",
       "   1.1564021872622625,\n",
       "   1.2162472712142127,\n",
       "   1.1870364636182784,\n",
       "   1.1766083968537195,\n",
       "   1.1866138649838311,\n",
       "   1.1479266456195287,\n",
       "   1.1695971148354667,\n",
       "   1.1564657224076136,\n",
       "   1.1479525757687432,\n",
       "   1.1707052482025964,\n",
       "   1.1855125980717796,\n",
       "   1.1487114242144993,\n",
       "   1.1916919729539326,\n",
       "   1.2108457037380764,\n",
       "   1.1814835629292897,\n",
       "   1.1745139249733516,\n",
       "   1.1834219549383436,\n",
       "   1.180319190451077,\n",
       "   1.2034488401242665,\n",
       "   1.2141141955341612,\n",
       "   1.194896801028933,\n",
       "   1.237115215403693,\n",
       "   1.2151524829013007,\n",
       "   1.1959327063390186,\n",
       "   1.242798708166395,\n",
       "   1.2168896181242808,\n",
       "   1.2418167671986988,\n",
       "   1.2057859535728181,\n",
       "   1.25539898616927,\n",
       "   1.2292798165764127,\n",
       "   1.2206744513341359,\n",
       "   1.2312892275197165,\n",
       "   1.203550986307008,\n",
       "   1.2452983383621488,\n",
       "   1.2560180825846536,\n",
       "   1.258358405317579,\n",
       "   1.2643525702612741],\n",
       "  [45.078125,\n",
       "   46.841517857142854,\n",
       "   50.770089285714285,\n",
       "   52.44419642857143,\n",
       "   54.933035714285715,\n",
       "   55.95982142857143,\n",
       "   57.25446428571429,\n",
       "   56.13839285714286,\n",
       "   57.566964285714285,\n",
       "   58.38169642857143,\n",
       "   57.042410714285715,\n",
       "   58.113839285714285,\n",
       "   58.41517857142857,\n",
       "   56.46205357142857,\n",
       "   57.46651785714286,\n",
       "   58.72767857142858,\n",
       "   57.332589285714285,\n",
       "   59.11830357142858,\n",
       "   58.113839285714285,\n",
       "   58.96205357142858,\n",
       "   59.53125,\n",
       "   58.783482142857146,\n",
       "   58.38169642857143,\n",
       "   58.59375,\n",
       "   58.32589285714286,\n",
       "   58.25892857142857,\n",
       "   59.45312499999999,\n",
       "   58.59375,\n",
       "   58.046875,\n",
       "   58.560267857142854,\n",
       "   57.74553571428571,\n",
       "   58.06919642857142,\n",
       "   57.99107142857143,\n",
       "   56.830357142857146,\n",
       "   57.75669642857143,\n",
       "   58.87276785714286,\n",
       "   57.34375000000001,\n",
       "   57.86830357142857,\n",
       "   57.957589285714285,\n",
       "   58.13616071428571,\n",
       "   58.10267857142857,\n",
       "   57.60044642857143,\n",
       "   58.560267857142854,\n",
       "   58.22544642857142,\n",
       "   58.995535714285715,\n",
       "   57.52232142857143,\n",
       "   57.421875,\n",
       "   57.89062499999999,\n",
       "   58.404017857142854],\n",
       "  [1.6928961923466392,\n",
       "   1.473445524030657,\n",
       "   1.368958224535158,\n",
       "   1.298225041857491,\n",
       "   1.2496767505399708,\n",
       "   1.2130982789403595,\n",
       "   1.1783092897160306,\n",
       "   1.155764786842959,\n",
       "   1.1270583545221196,\n",
       "   1.107730686749307,\n",
       "   1.0912135770410702,\n",
       "   1.072108325080492,\n",
       "   1.0567885847827094,\n",
       "   1.0398219789523306,\n",
       "   1.0285767701897286,\n",
       "   1.015164560388824,\n",
       "   1.0052335684657012,\n",
       "   0.995168689598661,\n",
       "   0.9858004366495801,\n",
       "   0.9723254483730639,\n",
       "   0.9619200823573076,\n",
       "   0.9531879013704763,\n",
       "   0.9449166330616781,\n",
       "   0.9389481891235224,\n",
       "   0.9323658910344997,\n",
       "   0.9219201428295453,\n",
       "   0.919481649327634,\n",
       "   0.9146163705484869,\n",
       "   0.9050407560480005,\n",
       "   0.9003272699648892,\n",
       "   0.8972665346422853,\n",
       "   0.8866044175734991,\n",
       "   0.8825778383774883,\n",
       "   0.8786184080712915,\n",
       "   0.877865949165084,\n",
       "   0.8703786144463322,\n",
       "   0.869130382109129,\n",
       "   0.8634999462621129,\n",
       "   0.8557211320156227,\n",
       "   0.8505565821700852,\n",
       "   0.8496190143461845,\n",
       "   0.8458367515389769,\n",
       "   0.8440408261218814,\n",
       "   0.8373886831448955,\n",
       "   0.8429602545122772,\n",
       "   0.8340116012748794,\n",
       "   0.8281657612527104,\n",
       "   0.8284857250014009,\n",
       "   0.8287059451428896],\n",
       "  [37.79222222222222,\n",
       "   46.23111111111111,\n",
       "   50.39444444444444,\n",
       "   53.08222222222222,\n",
       "   54.82222222222223,\n",
       "   56.30111111111111,\n",
       "   57.49666666666666,\n",
       "   58.282222222222224,\n",
       "   59.437777777777775,\n",
       "   60.21666666666666,\n",
       "   60.7011111111111,\n",
       "   61.55444444444444,\n",
       "   61.96333333333334,\n",
       "   62.35444444444445,\n",
       "   62.955555555555556,\n",
       "   63.35444444444445,\n",
       "   63.77555555555555,\n",
       "   64.07444444444444,\n",
       "   64.38000000000001,\n",
       "   64.99888888888889,\n",
       "   65.48222222222222,\n",
       "   65.69333333333334,\n",
       "   65.96888888888888,\n",
       "   66.33777777777777,\n",
       "   66.39444444444445,\n",
       "   66.7388888888889,\n",
       "   66.88444444444444,\n",
       "   66.93777777777777,\n",
       "   67.09222222222222,\n",
       "   67.46666666666667,\n",
       "   67.59888888888888,\n",
       "   68.0,\n",
       "   68.13777777777777,\n",
       "   68.31777777777778,\n",
       "   68.22,\n",
       "   68.6511111111111,\n",
       "   68.74666666666667,\n",
       "   68.80111111111111,\n",
       "   69.03444444444445,\n",
       "   69.29666666666667,\n",
       "   69.30777777777777,\n",
       "   69.37333333333333,\n",
       "   69.52888888888889,\n",
       "   69.74111111111111,\n",
       "   69.59666666666666,\n",
       "   69.79777777777778,\n",
       "   70.15666666666667,\n",
       "   69.90888888888888,\n",
       "   69.94222222222221],\n",
       "  1.2689947995020299,\n",
       "  58.02777777777778),\n",
       " (CNN_2(\n",
       "    (conv1): Conv2d(3, 16, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (linear1): Linear(in_features=1568, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  ),\n",
       "  0.001,\n",
       "  [1.5020665841443197,\n",
       "   1.396439527613776,\n",
       "   1.3071679822036197,\n",
       "   1.2883591490132469,\n",
       "   1.266763241801943,\n",
       "   1.2349447080067226,\n",
       "   1.2056762665510177,\n",
       "   1.2507376096078329,\n",
       "   1.1858392702681677,\n",
       "   1.2063859109367643,\n",
       "   1.2648063434021815,\n",
       "   1.2240419634750912,\n",
       "   1.2429431676864624,\n",
       "   1.266237223999841,\n",
       "   1.2941236342702593,\n",
       "   1.2776312346969332,\n",
       "   1.288469414625849,\n",
       "   1.2877127051353454,\n",
       "   1.2763634992497308,\n",
       "   1.3110097442354476,\n",
       "   1.2956355597291673,\n",
       "   1.331694951227733,\n",
       "   1.3887445901121411,\n",
       "   1.3323512251888003,\n",
       "   1.4687843714441573,\n",
       "   1.4156841869865144,\n",
       "   1.4492470341069357,\n",
       "   1.4510017969778606,\n",
       "   1.4929183955703462,\n",
       "   1.5170477794749395],\n",
       "  [45.078125,\n",
       "   49.00669642857143,\n",
       "   52.86830357142858,\n",
       "   53.20312499999999,\n",
       "   54.40848214285714,\n",
       "   55.52455357142857,\n",
       "   57.14285714285714,\n",
       "   55.63616071428571,\n",
       "   57.667410714285715,\n",
       "   56.50669642857142,\n",
       "   54.84375,\n",
       "   57.779017857142854,\n",
       "   57.10937499999999,\n",
       "   55.52455357142857,\n",
       "   54.16294642857142,\n",
       "   56.68526785714286,\n",
       "   55.60267857142858,\n",
       "   56.14955357142857,\n",
       "   56.46205357142857,\n",
       "   56.20535714285714,\n",
       "   56.395089285714285,\n",
       "   55.79241071428571,\n",
       "   55.111607142857146,\n",
       "   55.44642857142858,\n",
       "   55.02232142857143,\n",
       "   56.160714285714285,\n",
       "   55.3125,\n",
       "   55.92633928571429,\n",
       "   54.921875,\n",
       "   54.933035714285715],\n",
       "  [1.676762722194322,\n",
       "   1.4305611011739647,\n",
       "   1.3224312359759718,\n",
       "   1.2508716434981693,\n",
       "   1.1975443669473693,\n",
       "   1.157847812350286,\n",
       "   1.1228664364896095,\n",
       "   1.0943948312185297,\n",
       "   1.0670222485836465,\n",
       "   1.0398741653817887,\n",
       "   1.019323707304699,\n",
       "   0.9985510481107125,\n",
       "   0.9787690112077351,\n",
       "   0.9612644930299327,\n",
       "   0.9412616631327237,\n",
       "   0.926402012380032,\n",
       "   0.9081481353729942,\n",
       "   0.8909441073396651,\n",
       "   0.8769874932415196,\n",
       "   0.8613874778606914,\n",
       "   0.8440950584055772,\n",
       "   0.8309303211166067,\n",
       "   0.8158810435242914,\n",
       "   0.8037973592149169,\n",
       "   0.7862660975725666,\n",
       "   0.7751916700970135,\n",
       "   0.7595260069568528,\n",
       "   0.7489932214824037,\n",
       "   0.7371386370721572,\n",
       "   0.722339545083368],\n",
       "  [38.42777777777778,\n",
       "   47.871111111111105,\n",
       "   52.04555555555556,\n",
       "   54.96333333333333,\n",
       "   57.00333333333333,\n",
       "   58.45111111111111,\n",
       "   59.79444444444444,\n",
       "   60.68666666666667,\n",
       "   61.644444444444446,\n",
       "   62.739999999999995,\n",
       "   63.260000000000005,\n",
       "   64.24444444444445,\n",
       "   64.73222222222222,\n",
       "   65.50777777777778,\n",
       "   66.14,\n",
       "   66.80444444444444,\n",
       "   67.40666666666667,\n",
       "   67.96222222222222,\n",
       "   68.48777777777778,\n",
       "   69.01222222222222,\n",
       "   69.61444444444444,\n",
       "   69.96666666666667,\n",
       "   70.53,\n",
       "   71.09111111111112,\n",
       "   71.8711111111111,\n",
       "   71.96444444444444,\n",
       "   72.56555555555556,\n",
       "   73.01555555555555,\n",
       "   73.42,\n",
       "   74.0],\n",
       "  1.5166548662094166,\n",
       "  54.57333333333333),\n",
       " (CNN(\n",
       "    (conv1): Conv2d(3, 16, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (linear1): Linear(in_features=3600, out_features=200, bias=True)\n",
       "    (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
       "  ),\n",
       "  0.001,\n",
       "  [1.4766157805919646,\n",
       "   1.413522675207683,\n",
       "   1.3614758798054287,\n",
       "   1.3400835284164974,\n",
       "   1.2996854215860367,\n",
       "   1.3599944583007268,\n",
       "   1.300619610292571,\n",
       "   1.377566574726786,\n",
       "   1.3715894034930638,\n",
       "   1.4568343375410353,\n",
       "   1.4639676698616573,\n",
       "   1.4858755554471696,\n",
       "   1.5561176134007317],\n",
       "  [46.94196428571429,\n",
       "   48.16964285714286,\n",
       "   50.29017857142857,\n",
       "   51.43973214285714,\n",
       "   53.49330357142858,\n",
       "   52.51116071428571,\n",
       "   54.43080357142858,\n",
       "   52.79017857142857,\n",
       "   53.08035714285714,\n",
       "   51.595982142857146,\n",
       "   53.214285714285715,\n",
       "   53.33705357142857,\n",
       "   52.77901785714286],\n",
       "  [1.6183061592106118,\n",
       "   1.4108532504486377,\n",
       "   1.3117267768181378,\n",
       "   1.227513827367632,\n",
       "   1.1560013675909984,\n",
       "   1.0944404703298187,\n",
       "   1.0356418657319801,\n",
       "   0.9822263931584172,\n",
       "   0.9218775599919572,\n",
       "   0.8703467292656861,\n",
       "   0.8193651232510996,\n",
       "   0.7695215453513565,\n",
       "   0.7187428216859645],\n",
       "  [40.90555555555556,\n",
       "   48.96666666666666,\n",
       "   52.747777777777785,\n",
       "   55.85888888888889,\n",
       "   58.391111111111115,\n",
       "   60.68555555555556,\n",
       "   62.82555555555555,\n",
       "   64.79666666666667,\n",
       "   67.0288888888889,\n",
       "   68.81777777777778,\n",
       "   70.70333333333333,\n",
       "   72.62444444444445,\n",
       "   74.43111111111111],\n",
       "  1.569730815729861,\n",
       "  52.812222222222225)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dafa15f-7f25-4979-a66b-2732c585ac1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered the loop\n",
      "Epoch 1, Average Loss: 1.7332\n",
      "Epoch 1, Validation Loss: 1.5494, Accuracy: 42.95%\n",
      "Entered the loop\n",
      "Epoch 2, Average Loss: 1.5068\n",
      "Epoch 2, Validation Loss: 1.4610, Accuracy: 46.10%\n",
      "Entered the loop\n",
      "Epoch 3, Average Loss: 1.4023\n",
      "Epoch 3, Validation Loss: 1.3136, Accuracy: 51.95%\n",
      "Entered the loop\n",
      "Epoch 4, Average Loss: 1.3263\n",
      "Epoch 4, Validation Loss: 1.3305, Accuracy: 50.98%\n",
      "Entered the loop\n",
      "Epoch 5, Average Loss: 1.2694\n",
      "Epoch 5, Validation Loss: 1.2292, Accuracy: 55.36%\n",
      "Entered the loop\n",
      "Epoch 6, Average Loss: 1.2270\n",
      "Epoch 6, Validation Loss: 1.2025, Accuracy: 56.79%\n",
      "Entered the loop\n",
      "Epoch 7, Average Loss: 1.1915\n",
      "Epoch 7, Validation Loss: 1.1755, Accuracy: 57.37%\n",
      "Entered the loop\n",
      "Epoch 8, Average Loss: 1.1656\n",
      "Epoch 8, Validation Loss: 1.1764, Accuracy: 57.53%\n",
      "Entered the loop\n",
      "Epoch 9, Average Loss: 1.1413\n",
      "Epoch 9, Validation Loss: 1.1474, Accuracy: 58.42%\n",
      "Entered the loop\n",
      "Epoch 10, Average Loss: 1.1190\n",
      "Epoch 10, Validation Loss: 1.1413, Accuracy: 59.11%\n",
      "Entered the loop\n",
      "Epoch 11, Average Loss: 1.1018\n",
      "Epoch 11, Validation Loss: 1.1511, Accuracy: 58.33%\n",
      "Entered the loop\n",
      "Epoch 12, Average Loss: 1.0893\n",
      "Epoch 12, Validation Loss: 1.1274, Accuracy: 59.76%\n",
      "Entered the loop\n",
      "Epoch 13, Average Loss: 1.0740\n",
      "Epoch 13, Validation Loss: 1.1274, Accuracy: 59.75%\n",
      "Entered the loop\n",
      "Epoch 14, Average Loss: 1.0572\n",
      "Epoch 14, Validation Loss: 1.1633, Accuracy: 58.22%\n",
      "Entered the loop\n",
      "Epoch 15, Average Loss: 1.0485\n",
      "Epoch 15, Validation Loss: 1.1445, Accuracy: 58.50%\n",
      "Entered the loop\n",
      "Epoch 16, Average Loss: 1.0388\n",
      "Epoch 16, Validation Loss: 1.1244, Accuracy: 60.04%\n",
      "Entered the loop\n",
      "Epoch 17, Average Loss: 1.0233\n",
      "Epoch 17, Validation Loss: 1.1421, Accuracy: 58.95%\n",
      "Entered the loop\n",
      "Epoch 18, Average Loss: 1.0184\n",
      "Epoch 18, Validation Loss: 1.1021, Accuracy: 60.56%\n",
      "Entered the loop\n",
      "Epoch 19, Average Loss: 1.0061\n",
      "Epoch 19, Validation Loss: 1.0984, Accuracy: 60.56%\n",
      "Entered the loop\n",
      "Epoch 20, Average Loss: 0.9999\n",
      "Epoch 20, Validation Loss: 1.0867, Accuracy: 60.86%\n",
      "Entered the loop\n",
      "Epoch 21, Average Loss: 0.9932\n",
      "Epoch 21, Validation Loss: 1.0901, Accuracy: 60.65%\n",
      "Entered the loop\n",
      "Epoch 22, Average Loss: 0.9854\n",
      "Epoch 22, Validation Loss: 1.1040, Accuracy: 60.27%\n",
      "Entered the loop\n",
      "Epoch 23, Average Loss: 0.9774\n",
      "Epoch 23, Validation Loss: 1.0854, Accuracy: 60.92%\n",
      "Entered the loop\n",
      "Epoch 24, Average Loss: 0.9687\n",
      "Epoch 24, Validation Loss: 1.0858, Accuracy: 61.36%\n",
      "Entered the loop\n",
      "Epoch 25, Average Loss: 0.9601\n",
      "Epoch 25, Validation Loss: 1.1054, Accuracy: 60.55%\n",
      "Entered the loop\n",
      "Epoch 26, Average Loss: 0.9619\n",
      "Epoch 26, Validation Loss: 1.0988, Accuracy: 60.44%\n",
      "Entered the loop\n",
      "Epoch 27, Average Loss: 0.9529\n",
      "Epoch 27, Validation Loss: 1.0991, Accuracy: 60.94%\n",
      "Entered the loop\n",
      "Epoch 28, Average Loss: 0.9496\n",
      "Epoch 28, Validation Loss: 1.0897, Accuracy: 60.74%\n",
      "Entered the loop\n",
      "Epoch 29, Average Loss: 0.9396\n",
      "Epoch 29, Validation Loss: 1.0999, Accuracy: 60.76%\n",
      "Entered the loop\n",
      "Epoch 30, Average Loss: 0.9385\n",
      "Epoch 30, Validation Loss: 1.1078, Accuracy: 60.48%\n",
      "Entered the loop\n",
      "Epoch 31, Average Loss: 0.9381\n",
      "Epoch 31, Validation Loss: 1.1143, Accuracy: 60.29%\n",
      "Entered the loop\n",
      "Epoch 32, Average Loss: 0.9330\n",
      "Epoch 32, Validation Loss: 1.1279, Accuracy: 59.99%\n",
      " Test Loss: 1.1199, Test Accuracy: 60.38%\n",
      "Entered the loop\n",
      "Epoch 1, Average Loss: 1.6183\n",
      "Epoch 1, Validation Loss: 1.4836, Accuracy: 46.39%\n",
      "Entered the loop\n",
      "Epoch 2, Average Loss: 1.4109\n",
      "Epoch 2, Validation Loss: 1.4123, Accuracy: 48.66%\n",
      "Entered the loop\n",
      "Epoch 3, Average Loss: 1.3117\n",
      "Epoch 3, Validation Loss: 1.3672, Accuracy: 50.20%\n",
      "Entered the loop\n",
      "Epoch 4, Average Loss: 1.2275\n",
      "Epoch 4, Validation Loss: 1.3373, Accuracy: 51.50%\n",
      "Entered the loop\n",
      "Epoch 5, Average Loss: 1.1560\n",
      "Epoch 5, Validation Loss: 1.3056, Accuracy: 53.47%\n",
      "Entered the loop\n",
      "Epoch 6, Average Loss: 1.0944\n",
      "Epoch 6, Validation Loss: 1.3544, Accuracy: 52.61%\n",
      "Entered the loop\n",
      "Epoch 7, Average Loss: 1.0356\n",
      "Epoch 7, Validation Loss: 1.3058, Accuracy: 54.21%\n",
      "Entered the loop\n",
      "Epoch 8, Average Loss: 0.9822\n",
      "Epoch 8, Validation Loss: 1.3494, Accuracy: 53.51%\n",
      "Entered the loop\n",
      "Epoch 9, Average Loss: 0.9219\n",
      "Epoch 9, Validation Loss: 1.3647, Accuracy: 53.25%\n",
      "Entered the loop\n",
      "Epoch 10, Average Loss: 0.8703\n",
      "Epoch 10, Validation Loss: 1.4468, Accuracy: 51.70%\n",
      "Entered the loop\n",
      "Epoch 11, Average Loss: 0.8194\n",
      "Epoch 11, Validation Loss: 1.4454, Accuracy: 53.81%\n",
      "Entered the loop\n",
      "Epoch 12, Average Loss: 0.7695\n",
      "Epoch 12, Validation Loss: 1.4966, Accuracy: 52.68%\n",
      "Entered the loop\n",
      "Epoch 13, Average Loss: 0.7187\n",
      "Epoch 13, Validation Loss: 1.5697, Accuracy: 52.72%\n",
      "Entered the loop\n",
      "Epoch 14, Average Loss: 0.6737\n",
      "Epoch 14, Validation Loss: 1.6398, Accuracy: 52.02%\n",
      "Entered the loop\n",
      "Epoch 15, Average Loss: 0.6307\n",
      "Epoch 15, Validation Loss: 1.7001, Accuracy: 51.74%\n",
      " Test Loss: 1.7068, Test Accuracy: 51.84%\n",
      "Entered the loop\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m lrate:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:       \n\u001b[1;32m----> 6\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(model_train(lr,model,\u001b[38;5;241m42\u001b[39m))\n",
      "Cell \u001b[1;32mIn[11], line 41\u001b[0m, in \u001b[0;36mmodel_train\u001b[1;34m(lr, model, seed)\u001b[0m\n\u001b[0;32m     38\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     39\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (batch_X, batch_Y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[0;32m     42\u001b[0m     X \u001b[38;5;241m=\u001b[39m batch_X\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[0;32m     43\u001b[0m     Y \u001b[38;5;241m=\u001b[39m batch_Y\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \n",
      "File \u001b[1;32mD:\\anaconda\\envs\\torch_envi\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\torch_envi\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\torch_envi\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1401\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1402\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[0;32m   1403\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1404\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\torch_envi\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\torch_envi\\Lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\torch_envi\\Lib\\threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lrate = [0.001,0.0001]\n",
    "models= [CNN_2_dropout, CNN]\n",
    "results = []\n",
    "for lr in lrate:\n",
    "    for model in models:       \n",
    "        results.append(model_train(lr,model,42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b0ba5a1-ca21-4d92-96e8-b85688387910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(CNN_2_dropout(\n",
       "    (conv1): Conv2d(3, 16, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (linear1): Linear(in_features=576, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  ),\n",
       "  0.001,\n",
       "  [1.5493619407926287,\n",
       "   1.4610315348420824,\n",
       "   1.313620889186859,\n",
       "   1.330524436065129,\n",
       "   1.2291575023106167,\n",
       "   1.2025145377431596,\n",
       "   1.175507812840598,\n",
       "   1.176432757292475,\n",
       "   1.1473735579422542,\n",
       "   1.1413454996688026,\n",
       "   1.1510955899953843,\n",
       "   1.127378837125642,\n",
       "   1.1273821060146605,\n",
       "   1.1632507596697126,\n",
       "   1.144453969172069,\n",
       "   1.1243826040199825,\n",
       "   1.142071709036827,\n",
       "   1.1020530819892884,\n",
       "   1.098408264347485,\n",
       "   1.0866741240024567,\n",
       "   1.0900856290544783,\n",
       "   1.1040018213646752,\n",
       "   1.0854189021246774,\n",
       "   1.0858257974897112,\n",
       "   1.105383488535881,\n",
       "   1.098784608926092,\n",
       "   1.099103617668152,\n",
       "   1.0896634433950696,\n",
       "   1.099908135192735,\n",
       "   1.1077518509966986,\n",
       "   1.1142647513321469,\n",
       "   1.1279294827154704],\n",
       "  [42.94642857142857,\n",
       "   46.09933035714286,\n",
       "   51.953125,\n",
       "   50.982142857142854,\n",
       "   55.35714285714286,\n",
       "   56.79129464285714,\n",
       "   57.36607142857143,\n",
       "   57.527901785714285,\n",
       "   58.41517857142857,\n",
       "   59.10714285714286,\n",
       "   58.33147321428571,\n",
       "   59.760044642857146,\n",
       "   59.754464285714285,\n",
       "   58.21986607142857,\n",
       "   58.49888392857143,\n",
       "   60.03906249999999,\n",
       "   58.950892857142854,\n",
       "   60.56361607142857,\n",
       "   60.56361607142857,\n",
       "   60.86495535714286,\n",
       "   60.64732142857143,\n",
       "   60.26785714285714,\n",
       "   60.91517857142858,\n",
       "   61.35602678571429,\n",
       "   60.552455357142854,\n",
       "   60.440848214285715,\n",
       "   60.9375,\n",
       "   60.736607142857146,\n",
       "   60.76450892857142,\n",
       "   60.47991071428571,\n",
       "   60.29017857142858,\n",
       "   59.994419642857146],\n",
       "  [1.733208957426753,\n",
       "   1.5067539714970484,\n",
       "   1.4023439241116489,\n",
       "   1.3263025871810967,\n",
       "   1.269405670681149,\n",
       "   1.2270344832558622,\n",
       "   1.1915141438116144,\n",
       "   1.1656069040891543,\n",
       "   1.1413385722344025,\n",
       "   1.1190033046675643,\n",
       "   1.1017650415394098,\n",
       "   1.0892789690055658,\n",
       "   1.0740364735314587,\n",
       "   1.0572088980979757,\n",
       "   1.0484746218405467,\n",
       "   1.0388018860406882,\n",
       "   1.0232896906904236,\n",
       "   1.018432807990144,\n",
       "   1.0061334278791951,\n",
       "   0.9999289161021945,\n",
       "   0.9932238855680507,\n",
       "   0.9853856214497558,\n",
       "   0.9773637438040722,\n",
       "   0.9686789995148068,\n",
       "   0.9600811169006847,\n",
       "   0.9618550450478197,\n",
       "   0.952852647357065,\n",
       "   0.9496086307171819,\n",
       "   0.939551985763707,\n",
       "   0.9384599053952909,\n",
       "   0.9380993777839699,\n",
       "   0.9330249200412883],\n",
       "  [35.34777777777778,\n",
       "   44.28888888888889,\n",
       "   48.69222222222222,\n",
       "   51.54,\n",
       "   53.742222222222225,\n",
       "   55.50555555555555,\n",
       "   56.75555555555556,\n",
       "   57.88666666666666,\n",
       "   58.49888888888889,\n",
       "   59.324444444444445,\n",
       "   60.19222222222223,\n",
       "   60.581111111111106,\n",
       "   61.035555555555554,\n",
       "   61.83444444444445,\n",
       "   62.08444444444444,\n",
       "   62.56444444444445,\n",
       "   62.99444444444444,\n",
       "   63.06666666666667,\n",
       "   63.50444444444444,\n",
       "   63.71222222222222,\n",
       "   63.974444444444444,\n",
       "   64.19444444444444,\n",
       "   64.52,\n",
       "   64.99333333333334,\n",
       "   65.24222222222222,\n",
       "   65.00777777777778,\n",
       "   65.55333333333333,\n",
       "   65.57777777777778,\n",
       "   65.80333333333334,\n",
       "   65.8288888888889,\n",
       "   65.84,\n",
       "   66.13888888888889],\n",
       "  1.119912345089357,\n",
       "  60.38333333333333),\n",
       " (CNN(\n",
       "    (conv1): Conv2d(3, 16, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (linear1): Linear(in_features=3600, out_features=200, bias=True)\n",
       "    (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
       "  ),\n",
       "  0.001,\n",
       "  [1.4835867030279977,\n",
       "   1.412284643309457,\n",
       "   1.3672196209430694,\n",
       "   1.337349534034729,\n",
       "   1.3056229421070644,\n",
       "   1.3544082496847425,\n",
       "   1.3058143649782454,\n",
       "   1.3494190914290292,\n",
       "   1.3647250754492624,\n",
       "   1.4467669444424764,\n",
       "   1.445407214335033,\n",
       "   1.4965524234942027,\n",
       "   1.5696956617491586,\n",
       "   1.6398403125149863,\n",
       "   1.7000877346311296],\n",
       "  [46.38950892857142,\n",
       "   48.660714285714285,\n",
       "   50.1953125,\n",
       "   51.50111607142858,\n",
       "   53.465401785714285,\n",
       "   52.61160714285714,\n",
       "   54.207589285714285,\n",
       "   53.510044642857146,\n",
       "   53.24776785714286,\n",
       "   51.70200892857143,\n",
       "   53.80580357142857,\n",
       "   52.684151785714285,\n",
       "   52.71763392857143,\n",
       "   52.02008928571429,\n",
       "   51.73549107142858],\n",
       "  [1.6183061592106118,\n",
       "   1.4108532504486377,\n",
       "   1.3117267768181378,\n",
       "   1.227513827367632,\n",
       "   1.1560013675909984,\n",
       "   1.0944404703298187,\n",
       "   1.0356418657319801,\n",
       "   0.9822263931584172,\n",
       "   0.9218775599919572,\n",
       "   0.8703467292656861,\n",
       "   0.8193651232510996,\n",
       "   0.7695215453513565,\n",
       "   0.7187428216859645,\n",
       "   0.673729964474371,\n",
       "   0.6306857458860551],\n",
       "  [40.90555555555556,\n",
       "   48.96666666666666,\n",
       "   52.747777777777785,\n",
       "   55.85888888888889,\n",
       "   58.391111111111115,\n",
       "   60.68555555555556,\n",
       "   62.82555555555555,\n",
       "   64.79666666666667,\n",
       "   67.0288888888889,\n",
       "   68.81777777777778,\n",
       "   70.70333333333333,\n",
       "   72.62444444444445,\n",
       "   74.43111111111111,\n",
       "   76.07000000000001,\n",
       "   77.7388888888889],\n",
       "  1.7067508023994213,\n",
       "  51.83777777777778)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cfb56b-2913-46dc-ae02-65e87dcb2d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
